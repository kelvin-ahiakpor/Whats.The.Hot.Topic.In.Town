# -*- coding: utf-8 -*-
"""Phase2_Phase3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DnJzMAbJcsZ9ucJrdDsDey2SWUpACJzI
"""
from transformers import pipeline
import re
import sqlite3
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS
from collections import Counter
import streamlit as st

# Load the sentiment analysis pipeline
sentiment_analyzer = pipeline("sentiment-analysis")

def load_and_clean_data(database):
    newsdata = pd.read_sql('SELECT * FROM news', database)
    newsdata = newsdata.dropna() 
    return newsdata

# Function to split text into sentences
def split_text_by_sentences(text):
    sentences = re.split(r'(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.|\?)\s', text)
    return sentences

# Preprocess the data
def preprocess(text):
    text = text.lower()  # Lowercase
    # Add more preprocessing steps if needed
    return text

def get_top_terms(newsdata, num_features=1000, top_n=20):
    newsdata['cleaned_body'] = newsdata['BODY'].apply(preprocess)
    
    vectorizer = TfidfVectorizer(stop_words=list(ENGLISH_STOP_WORDS), max_features=num_features)
    X = vectorizer.fit_transform(newsdata['cleaned_body'])
    
    feature_names = vectorizer.get_feature_names_out()
    tfidf_scores = X.sum(axis=0).A1
    term_scores = dict(zip(feature_names, tfidf_scores))
    
    top_terms = Counter(term_scores).most_common(top_n)
    return top_terms

def calculate_relevance(newsdata, top_terms):
    article_scores = []
    
    for index, row in newsdata.iterrows():
        relevance_score = 0
        for term, _ in top_terms:
            relevance_score += row['cleaned_body'].count(term)  # Count occurrences of the term
        article_scores.append({'TITLE': row['TITLE'],
                               'URL': row['Website Link'],
                               'BODY': row['BODY'],
                               'Relevance': relevance_score})
    
    article_scores_df = pd.DataFrame(article_scores)
    top_articles = article_scores_df.sort_values(by='Relevance', ascending=False).head(10)
    
    return top_articles

def analyze_sentiment(top_articles):
    text = top_articles
    sentences = split_text_by_sentences(text)
    results = []

    # Analyze each sentence and calculate the average sentiment score
    for sentence in sentences:
        result = sentiment_analyzer(sentence)[0]
        results.append(result)

    # Calculate the average sentiment score
    positive_scores = [result['score'] for result in results if result['label'] == 'POSITIVE']
    negative_scores = [result['score'] for result in results if result['label'] == 'NEGATIVE']

    average_positive = sum(positive_scores) / len(positive_scores) if positive_scores else 0
    average_negative = sum(negative_scores) / len(negative_scores) if negative_scores else 0

    # Determine the overall average sentiment
    overall_sentiment = "POSITIVE" if average_positive >= average_negative else "NEGATIVE"
    average_score = max(average_positive, average_negative)

    '''for index, row in top_articles.iterrows():
        if text in row['BODY']:
            print()
            print("TITLE OF ARTICLE: " + row['TITLE'])
            '''

    print(f"Overall Sentiment: {overall_sentiment}")
    print(f"Average Sentiment Score: {average_score:.4f}")
    
    return overall_sentiment

def main(database):
    newsdata = load_and_clean_data(database)
    top_terms = get_top_terms(newsdata)
    top_articles = calculate_relevance(newsdata, top_terms)