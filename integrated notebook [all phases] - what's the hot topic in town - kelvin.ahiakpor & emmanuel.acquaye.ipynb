{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ce74f137"
   },
   "source": [
    "# Project: What's The Hot Topic In Town? - kelvin.ahiakpor & emmanuel.acquaye"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook integrates Phase 1-5 of the What's The Hot Topic in Topic Town? project:   \n",
    "**1. [Web Scraping](#Phase_1)**  \n",
    "**2. [Display News](#Phase_2)**  \n",
    "**3. [Social Sentiment Analysis](#Phase_3)**  \n",
    "**4. [Summarize News Articles](#Phase_4)**  \n",
    "**5. [Deploy Functional App](#Phase_5)**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1a765bda"
   },
   "source": [
    "### Information Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1           \n",
    "Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The self-created rubric, in our repository, explains the requirement for a proper execution of this phase as seen below.   \n",
    "**Description:** Build a Spider to scrape news from at least 1 credible news source (AllAfrica, BBC, The New\n",
    "York Times, Reuters, Foreign Affairs).  \n",
    "**Note:** This notebook **only** shows the code and instructions for web scraping with a spider with .py files. Scrapy is built in python and will not run in this environment. To learn how to build a scrapy spider, read the Scrapy documentation. [Scrapy Tutorial](https://docs.scrapy.org/en/latest/intro/tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repository Link \n",
    "\n",
    "Here is a link to our repository:\n",
    "\n",
    "[What's The Hot Topic In Town?](https://github.com/kelvin-ahiakpor/Whats.The.Hot.Topic.In.Town)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "de02dd80"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import pycountry\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import sqlite3 as sql\n",
    "from scrapy.downloadermiddlewares.useragent import UserAgentMiddleware"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310,
     "referenced_widgets": [
      "ac85b1d108bf49bb90e06ed1152bc3c2",
      "a2d3e73f49174038bc66c5ec76100136",
      "e258fea5193b4c47889a84a67b6a4f21",
      "0e2ad5f651f84f9987ea84a6830a6ed6",
      "00a8884ce6454401bb291b39ca8e0640",
      "db1bee522cb845598c068e0b5213adea",
      "9fd470bbe627496690affb852ab46b07",
      "641f2d82451945708719f1ecab612bc7",
      "0126020315744923ba10f565fd625764",
      "7cb633e044c84c43b84ef52d8ed97025",
      "13b15a58c86645369040862f18973e14",
      "a24bd63ca7534fc39100939b1d09047d",
      "7f4a1e468221468b99b0b38817cbbc83",
      "c481a43995a84b889ff893ca7dfe155b",
      "13a7b31714e541f3bef75da2e1179e33",
      "0db04860235b468eaa0d67a4e0b75847",
      "c2c3d760acd749db992e7d9d83d28d20",
      "343499130b724c7a86412ac994eec587",
      "2088fac2762041ff8fffeee2293dd73a",
      "4e68225ed1334e99afdc90ae6422a230",
      "c8e158e13059418c8d35b15efe8f6674",
      "eb779fa3d1a34ea087d0a92e3dff67eb",
      "058f4eb2dc7641459c026c3ada509254",
      "8ff4d107dd4b42a0a2f8052cf0ebf575",
      "8e5c264f5c374bacbffea5e87b8e0126",
      "353d35c2f1264e41b1f732d52cf2113f",
      "88daaab7bdba4ca587dfba03945bf762",
      "716f02820db448c68b73c5380a20dc0b",
      "a5ab3da857eb4632b16bf4ea370ba662",
      "5ff8b153e480435f9277b384e13c2327",
      "02a9c20377b84d739804fb3ec99736ad",
      "7e4c8e1550f049068ca5c20263a9e6d8",
      "705ffb66e5e44ba6a698569c40fdabc0",
      "419c37febb194e95ba1ce6a10e02d702",
      "292aa3e3c77f4396a83fcd26152ca2ec",
      "f7a05fe737624af88bafc343f8a8d8e9",
      "4ff83a4439bd4420a42f5096fde41e2d",
      "fbc93bf0ecd94d339cdf2b80be2c2f7a",
      "3d8e59fd90cf40369de1c7c700a0236e",
      "a19fe100eca94cc3aa3a6a8921a2e936",
      "2346d4eb65f6428095056b1bb0afbdad",
      "702dd4ee8f7844828db5e9faf4deb5a3",
      "5ca9c213a9954b3c843828f9f86c0807",
      "d26e3d05d20d48639367f4347d0c6e50"
     ]
    },
    "id": "dFLbweF48w0d",
    "outputId": "8bb08bbe-57c0-4262-c72b-206652189911"
   },
   "source": [
    "**Create a .py file called** `news_scraper.py` **for create Spider classes**   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310,
     "referenced_widgets": [
      "ac85b1d108bf49bb90e06ed1152bc3c2",
      "a2d3e73f49174038bc66c5ec76100136",
      "e258fea5193b4c47889a84a67b6a4f21",
      "0e2ad5f651f84f9987ea84a6830a6ed6",
      "00a8884ce6454401bb291b39ca8e0640",
      "db1bee522cb845598c068e0b5213adea",
      "9fd470bbe627496690affb852ab46b07",
      "641f2d82451945708719f1ecab612bc7",
      "0126020315744923ba10f565fd625764",
      "7cb633e044c84c43b84ef52d8ed97025",
      "13b15a58c86645369040862f18973e14",
      "a24bd63ca7534fc39100939b1d09047d",
      "7f4a1e468221468b99b0b38817cbbc83",
      "c481a43995a84b889ff893ca7dfe155b",
      "13a7b31714e541f3bef75da2e1179e33",
      "0db04860235b468eaa0d67a4e0b75847",
      "c2c3d760acd749db992e7d9d83d28d20",
      "343499130b724c7a86412ac994eec587",
      "2088fac2762041ff8fffeee2293dd73a",
      "4e68225ed1334e99afdc90ae6422a230",
      "c8e158e13059418c8d35b15efe8f6674",
      "eb779fa3d1a34ea087d0a92e3dff67eb",
      "058f4eb2dc7641459c026c3ada509254",
      "8ff4d107dd4b42a0a2f8052cf0ebf575",
      "8e5c264f5c374bacbffea5e87b8e0126",
      "353d35c2f1264e41b1f732d52cf2113f",
      "88daaab7bdba4ca587dfba03945bf762",
      "716f02820db448c68b73c5380a20dc0b",
      "a5ab3da857eb4632b16bf4ea370ba662",
      "5ff8b153e480435f9277b384e13c2327",
      "02a9c20377b84d739804fb3ec99736ad",
      "7e4c8e1550f049068ca5c20263a9e6d8",
      "705ffb66e5e44ba6a698569c40fdabc0",
      "419c37febb194e95ba1ce6a10e02d702",
      "292aa3e3c77f4396a83fcd26152ca2ec",
      "f7a05fe737624af88bafc343f8a8d8e9",
      "4ff83a4439bd4420a42f5096fde41e2d",
      "fbc93bf0ecd94d339cdf2b80be2c2f7a",
      "3d8e59fd90cf40369de1c7c700a0236e",
      "a19fe100eca94cc3aa3a6a8921a2e936",
      "2346d4eb65f6428095056b1bb0afbdad",
      "702dd4ee8f7844828db5e9faf4deb5a3",
      "5ca9c213a9954b3c843828f9f86c0807",
      "d26e3d05d20d48639367f4347d0c6e50"
     ]
    },
    "id": "dFLbweF48w0d",
    "outputId": "8bb08bbe-57c0-4262-c72b-206652189911",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class TrendsNewsSpider(scrapy.Spider):\n",
    "    name = 'trends_news'\n",
    "    \n",
    "    custom_settings = {\n",
    "        'DOWNLOAD_DELAY': 2,  # Default delay of 2 seconds\n",
    "        'RANDOMIZE_DOWNLOAD_DELAY': True,\n",
    "        'CONCURRENT_REQUESTS': 1,\n",
    "        'RETRY_ENABLED': True,\n",
    "        'RETRY_TIMES': 3,\n",
    "        'DOWNLOADER_MIDDLEWARES': {\n",
    "            'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,\n",
    "            'newsspider.middlewares.RandomUserAgentMiddleware': 400,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(TrendsNewsSpider, self).__init__(*args, **kwargs)\n",
    "        self.data = []\n",
    "\n",
    "    def start_requests(self):\n",
    "        country = getattr(self, 'country', None)\n",
    "        if country is not None:\n",
    "            if ' ' in country:\n",
    "                country1 = country.replace(' ', '-')\n",
    "                url = 'https://trends24.in/' + country1.lower() + '/'\n",
    "            else:\n",
    "                url = 'https://trends24.in/' + country.lower() + '/'\n",
    "            url_allafrica = f'https://allafrica.com/{country.lower()}/'\n",
    "            \n",
    "            yield scrapy.Request(url=url, callback=self.parse_trends, meta={'country': country})\n",
    "            yield scrapy.Request(url=url_allafrica, callback=self.parse_allafrica, meta={'country': country})\n",
    "        else:\n",
    "            self.log(\"No country provided\")\n",
    "\n",
    "    def parse_trends(self, response):\n",
    "        country = response.meta['country']\n",
    "        trend_list = response.css('.list-container .trend-card__list')\n",
    "        trends = []\n",
    "        for trend in trend_list[0].css('li a::text').getall():\n",
    "            trends.append(trend.strip())\n",
    "        \n",
    "        self.log(f\"Top 50 trends in {country}:\")\n",
    "        for trend in trends[:50]:\n",
    "            self.log(trend)\n",
    "            #time.sleep(random.uniform(1, 3))  # Random delay between requests\n",
    "    \n",
    "    def parse_allafrica(self, response):\n",
    "        country = response.meta['country']\n",
    "        self.log(f\"Scraping AllAfrica for {country}\")\n",
    "        \n",
    "        articles = response.css('.container.mid .row .col-tn-12.col-sm-8.column.main .section.box.headlines.two-column .content .stories li a')\n",
    "        \n",
    "        for article in articles:\n",
    "            title = article.css('::attr(title)').get()\n",
    "            link = article.css('::attr(href)').get()\n",
    "            \n",
    "            if link and not link.startswith('http'):\n",
    "                link = response.urljoin(link)\n",
    "            \n",
    "            yield scrapy.Request(url=link, callback=self.parse_article, meta={'title': title, 'country': country})\n",
    "            time.sleep(random.uniform(0.5, 1))  # Random delay between requests\n",
    "\n",
    "    def parse_article(self, response):\n",
    "        title = response.meta['title']\n",
    "        country = response.meta['country']\n",
    "        \n",
    "        paragraphs = response.css('.container.mid .row .col-tn-12.col-sm-8.column.main .story-body p::text').getall()\n",
    "        \n",
    "        body = ' '.join(paragraphs).strip()\n",
    "        \n",
    "        org_link = response.css('.container.mid .row .col-tn-12.col-sm-8.column.main .story-footer-link .source-url::attr(href)').get()\n",
    "\n",
    "        self.data.append({\n",
    "            'TITLE': title,\n",
    "            'COUNTRY': country,\n",
    "            'BODY': body,\n",
    "            'Website Link': org_link\n",
    "        })\n",
    "\n",
    "    def closed(self, reason):\n",
    "        df = pd.DataFrame(self.data)\n",
    "        conn = sql.connect('newsData.db')\n",
    "        df.to_csv('newsData.csv', index=False)\n",
    "        df.to_sql('news', conn, if_exists='replace', index=False)\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a .py file called runspider to run the spider**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy.crawler import CrawlerRunner\n",
    "from scrapy.utils.project import get_project_settings\n",
    "from twisted.internet import reactor\n",
    "from twisted.internet.defer import inlineCallbacks\n",
    "from newsspider.spiders.news_scraper import TrendsNewsSpider\n",
    "from scrapy.utils.log import configure_logging\n",
    "import sys\n",
    "\n",
    "# Configure logging for Scrapy\n",
    "configure_logging()\n",
    "\n",
    "@inlineCallbacks\n",
    "def crawl(runner, country):\n",
    "    yield runner.crawl(TrendsNewsSpider, country=country)\n",
    "    reactor.stop()\n",
    "\n",
    "def run_spider(country):\n",
    "    runner = CrawlerRunner(get_project_settings())\n",
    "    crawl(runner, country)\n",
    "    reactor.run(installSignalHandlers=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    country = sys.argv[1]\n",
    "    run_spider(country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1a765bda"
   },
   "source": [
    "### Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2           \n",
    "Display News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The self-created rubric, in our repository, explains the requirement for a proper execution of this phase as seen below.   \n",
    "**Description:** Show top 10 news headlines with one paragraph summaries and allow user to select and\n",
    "read desired articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repository Link \n",
    "\n",
    "Here is a link to our repository:\n",
    "\n",
    "[What's The Hot Topic In Town?](https://github.com/kelvin-ahiakpor/Whats.The.Hot.Topic.In.Town)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "de02dd80"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets \n",
    "\n",
    "from collections import Counter\n",
    "from transformers import pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310,
     "referenced_widgets": [
      "ac85b1d108bf49bb90e06ed1152bc3c2",
      "a2d3e73f49174038bc66c5ec76100136",
      "e258fea5193b4c47889a84a67b6a4f21",
      "0e2ad5f651f84f9987ea84a6830a6ed6",
      "00a8884ce6454401bb291b39ca8e0640",
      "db1bee522cb845598c068e0b5213adea",
      "9fd470bbe627496690affb852ab46b07",
      "641f2d82451945708719f1ecab612bc7",
      "0126020315744923ba10f565fd625764",
      "7cb633e044c84c43b84ef52d8ed97025",
      "13b15a58c86645369040862f18973e14",
      "a24bd63ca7534fc39100939b1d09047d",
      "7f4a1e468221468b99b0b38817cbbc83",
      "c481a43995a84b889ff893ca7dfe155b",
      "13a7b31714e541f3bef75da2e1179e33",
      "0db04860235b468eaa0d67a4e0b75847",
      "c2c3d760acd749db992e7d9d83d28d20",
      "343499130b724c7a86412ac994eec587",
      "2088fac2762041ff8fffeee2293dd73a",
      "4e68225ed1334e99afdc90ae6422a230",
      "c8e158e13059418c8d35b15efe8f6674",
      "eb779fa3d1a34ea087d0a92e3dff67eb",
      "058f4eb2dc7641459c026c3ada509254",
      "8ff4d107dd4b42a0a2f8052cf0ebf575",
      "8e5c264f5c374bacbffea5e87b8e0126",
      "353d35c2f1264e41b1f732d52cf2113f",
      "88daaab7bdba4ca587dfba03945bf762",
      "716f02820db448c68b73c5380a20dc0b",
      "a5ab3da857eb4632b16bf4ea370ba662",
      "5ff8b153e480435f9277b384e13c2327",
      "02a9c20377b84d739804fb3ec99736ad",
      "7e4c8e1550f049068ca5c20263a9e6d8",
      "705ffb66e5e44ba6a698569c40fdabc0",
      "419c37febb194e95ba1ce6a10e02d702",
      "292aa3e3c77f4396a83fcd26152ca2ec",
      "f7a05fe737624af88bafc343f8a8d8e9",
      "4ff83a4439bd4420a42f5096fde41e2d",
      "fbc93bf0ecd94d339cdf2b80be2c2f7a",
      "3d8e59fd90cf40369de1c7c700a0236e",
      "a19fe100eca94cc3aa3a6a8921a2e936",
      "2346d4eb65f6428095056b1bb0afbdad",
      "702dd4ee8f7844828db5e9faf4deb5a3",
      "5ca9c213a9954b3c843828f9f86c0807",
      "d26e3d05d20d48639367f4347d0c6e50"
     ]
    },
    "id": "dFLbweF48w0d",
    "outputId": "8bb08bbe-57c0-4262-c72b-206652189911"
   },
   "source": [
    "**Load the default sentiment analysis pipeline**   \n",
    "`distilbert-base-uncased-finetuned-sst-2-english`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310,
     "referenced_widgets": [
      "ac85b1d108bf49bb90e06ed1152bc3c2",
      "a2d3e73f49174038bc66c5ec76100136",
      "e258fea5193b4c47889a84a67b6a4f21",
      "0e2ad5f651f84f9987ea84a6830a6ed6",
      "00a8884ce6454401bb291b39ca8e0640",
      "db1bee522cb845598c068e0b5213adea",
      "9fd470bbe627496690affb852ab46b07",
      "641f2d82451945708719f1ecab612bc7",
      "0126020315744923ba10f565fd625764",
      "7cb633e044c84c43b84ef52d8ed97025",
      "13b15a58c86645369040862f18973e14",
      "a24bd63ca7534fc39100939b1d09047d",
      "7f4a1e468221468b99b0b38817cbbc83",
      "c481a43995a84b889ff893ca7dfe155b",
      "13a7b31714e541f3bef75da2e1179e33",
      "0db04860235b468eaa0d67a4e0b75847",
      "c2c3d760acd749db992e7d9d83d28d20",
      "343499130b724c7a86412ac994eec587",
      "2088fac2762041ff8fffeee2293dd73a",
      "4e68225ed1334e99afdc90ae6422a230",
      "c8e158e13059418c8d35b15efe8f6674",
      "eb779fa3d1a34ea087d0a92e3dff67eb",
      "058f4eb2dc7641459c026c3ada509254",
      "8ff4d107dd4b42a0a2f8052cf0ebf575",
      "8e5c264f5c374bacbffea5e87b8e0126",
      "353d35c2f1264e41b1f732d52cf2113f",
      "88daaab7bdba4ca587dfba03945bf762",
      "716f02820db448c68b73c5380a20dc0b",
      "a5ab3da857eb4632b16bf4ea370ba662",
      "5ff8b153e480435f9277b384e13c2327",
      "02a9c20377b84d739804fb3ec99736ad",
      "7e4c8e1550f049068ca5c20263a9e6d8",
      "705ffb66e5e44ba6a698569c40fdabc0",
      "419c37febb194e95ba1ce6a10e02d702",
      "292aa3e3c77f4396a83fcd26152ca2ec",
      "f7a05fe737624af88bafc343f8a8d8e9",
      "4ff83a4439bd4420a42f5096fde41e2d",
      "fbc93bf0ecd94d339cdf2b80be2c2f7a",
      "3d8e59fd90cf40369de1c7c700a0236e",
      "a19fe100eca94cc3aa3a6a8921a2e936",
      "2346d4eb65f6428095056b1bb0afbdad",
      "702dd4ee8f7844828db5e9faf4deb5a3",
      "5ca9c213a9954b3c843828f9f86c0807",
      "d26e3d05d20d48639367f4347d0c6e50"
     ]
    },
    "id": "dFLbweF48w0d",
    "outputId": "8bb08bbe-57c0-4262-c72b-206652189911",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "sentiment_analyzer = pipeline(\"sentiment-analysis\", \n",
    "                              model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Widget creation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_widget = widgets.FileUpload(multiple=False)\n",
    "upload_button = widgets.Button(description=\"Process Upload\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13996d20-5eac-4ed8-a2f7-a3cff5e103e9"
   },
   "source": [
    "### Upload a news data csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a set of functions to allow uploads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13996d20-5eac-4ed8-a2f7-a3cff5e103e9"
   },
   "source": [
    "**The following set of functions allow uploads in both Jupyter and Google Colab.**  \n",
    "**Note:** This is for demonstration purposes only.  \n",
    "In our deployed app this process is automated where the news data csv is saved to a sqllite database after scraping with scrapy and is loaded back into the application to perform natural language processing and identify the top 10 news articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Global variables**  \n",
    "Dictionary of upload information   \n",
    "Video path  \n",
    "Upload Widget\n",
    "Upload Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_info = None\n",
    "file_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_completed = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_file_upload(uploaded_files):\n",
    "    for filename, file_info in uploaded_files.items():\n",
    "        save_file(filename, file_info['content'])\n",
    "        return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(filename, file_content):\n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(file_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files(button):\n",
    "    global upload_completed\n",
    "    global file_path\n",
    "    uploaded_files = upload_widget.value\n",
    "    # Handle file validation and save\n",
    "    file_path = validate_file_upload(uploaded_files)\n",
    "    if file_path:\n",
    "        print(f\"File path set to: {file_path}\")\n",
    "        upload_completed = True  # Mark the upload as complete\n",
    "    else:\n",
    "        print(\"No valid file uploaded.\")\n",
    "    \n",
    "    if upload_completed:\n",
    "        print()\n",
    "        print(\"Upload complete. You can now proceed to the run the next cells.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_upload_widget():\n",
    "    global upload_button\n",
    "    upload_button = widgets.Button(description=\"Upload File\")\n",
    "    upload_button.on_click(process_files)\n",
    "    \n",
    "    # Display the widgets\n",
    "    display(upload_widget)\n",
    "    display(upload_button)\n",
    "    \n",
    "    print(\"Please upload a file and then click the 'Upload File' button.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7f736c930cf4312ada17a2b6a15a010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value={}, description='Upload')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e17699c55b654de792a2a32d5999e5d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Upload File', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please upload a file and then click the 'Upload File' button.\n",
      "File path set to: newsData.csv\n",
      "\n",
      "Upload complete. You can now proceed to the run the next cells.\n"
     ]
    }
   ],
   "source": [
    "show_upload_widget()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the uploaded news data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>COUNTRY</th>\n",
       "      <th>BODY</th>\n",
       "      <th>Website Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Zimbabwean in South Africa Challenges Exorbita...</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>A Zimbabwean resident in South Africa, Takudzw...</td>\n",
       "      <td>https://www.newzimbabwe.com/zimbabwean-in-sout...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>American Star Tiffany Haddish Gives Zimbabwe a...</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>VISITING American comedienne, Tiffany Haddish,...</td>\n",
       "      <td>https://www.herald.co.zw/american-star-tiffany...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Insect Farming New Frontier for Smallholder Li...</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>Zimbabwe, just like many other countries, is l...</td>\n",
       "      <td>https://www.herald.co.zw/insect-farming-new-fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Zanu-PF, Swapo in Key Engagement</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>ZANU PF and SWAPO of Namibia share common bond...</td>\n",
       "      <td>https://www.herald.co.zw/zanu-pf-swapo-in-key-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Zanu-PF Official in Court Over $4 Million Frau...</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>A senior official from Zimbabwe's ruling Zanu ...</td>\n",
       "      <td>https://www.263chat.com/zanu-pf-official-in-co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Despite Hostility, LGBTQI+ Activists in Zimbab...</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>Munich, Germany —  LGBTQI+ people have long be...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Zim Swimmer Weisthuizen Out of the Olympics</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>ZIMBABWE swimmer Paige Van Der Weisthuizen was...</td>\n",
       "      <td>https://www.newzimbabwe.com/zim-swimmer-weisth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Dynamos' Mangombe in a Fix</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>Castle Lager Premiership giants, Dynamos head ...</td>\n",
       "      <td>https://www.herald.co.zw/dynamos-mangombe-in-a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>UN Tourism Delegation Visits First Lady</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>Victoria Falls — United Nations Tourism Secret...</td>\n",
       "      <td>https://www.herald.co.zw/un-tourism-delegation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>VP Chiwenga Rallies SADC to Boost Trade</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>SADC countries must use their regional advanta...</td>\n",
       "      <td>https://www.herald.co.zw/vp-chiwenga-rallies-s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>VP Mohadi in Iran for Pezeshkian Inauguration</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>Tehran, Iran — Vice President Kembo Mohadi has...</td>\n",
       "      <td>https://www.herald.co.zw/vp-mohadi-in-iran-for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Wheat Clusters Take Shape</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>CLUSTERING of wheat farmers for easy service d...</td>\n",
       "      <td>https://www.herald.co.zw/wheat-clusters-take-s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Tyla Captivates Crowd</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>Johannesburg. — Tyla captivated the Paris crow...</td>\n",
       "      <td>https://www.herald.co.zw/tyla-captivates-crowd/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Tourism Receipts Up 35pc to U.S.$241m</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>Zimbabwe's tourism receipts grew by an estimat...</td>\n",
       "      <td>https://www.herald.co.zw/tourism-receipts-up-3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Air Zim Resumes Harare-Joburg Flights</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>Air Zimbabwe yesterday resumed flights from Ha...</td>\n",
       "      <td>https://www.herald.co.zw/air-zim-resumes-harar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Air Ambulances Training Starts. . .300 Special...</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>THE Russian HeliDrive Air ambulances medical s...</td>\n",
       "      <td>https://www.herald.co.zw/air-ambulances-traini...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>AfCFTA Pursues Exports for Africa's Food Security</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>THE African Continental Free Trade Area (AfCFT...</td>\n",
       "      <td>https://www.herald.co.zw/afcfta-pursues-export...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>The Essential Role of Liability Insurance</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>Liability insurance is a critical safeguard fo...</td>\n",
       "      <td>https://www.herald.co.zw/the-essential-role-of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Sables Lift Up Nation's Spirits</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>After breaking a couple of jinxes to be crowne...</td>\n",
       "      <td>https://www.herald.co.zw/sables-lift-up-nation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Afreximbank Earmarks U.S.$400m for Zim Firms</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>The African Export-Import Bank (Afreximbank) h...</td>\n",
       "      <td>https://www.herald.co.zw/afreximbank-earmarks-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Presidential Solar Scheme Innovative, Imaginative</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>This is an installation by Zimbabwe Solar Ener...</td>\n",
       "      <td>https://www.herald.co.zw/editorial-comment-pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>US Diplomat Runs Over 11-Year-Old Girl, Flees ...</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>A United States Embassy official allegedly ran...</td>\n",
       "      <td>https://www.herald.co.zw/us-diplomat-runs-over...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Police Hunt for 8 Suspects</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>POLICE in Harare have launched a manhunt for e...</td>\n",
       "      <td>https://www.herald.co.zw/police-hunt-for-8-sus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Adachi Finding Form</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>Adachi, who had a poor start to the Zifa South...</td>\n",
       "      <td>https://www.herald.co.zw/adachi-finding-form/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>'Scrapping Vat On Cattle Sales Will Unlock Tru...</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>Stakeholders in the beef industry have describ...</td>\n",
       "      <td>https://www.herald.co.zw/scrapping-vat-on-catt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ZRP Bans Dangerous Weapons in Beitbridge</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>The Zimbabwe Republic Police (ZRP) has issued ...</td>\n",
       "      <td>https://www.herald.co.zw/zrp-bans-dangerous-we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Macheso Names 'Kupa Kuturika' Album Tracklist</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>ALICK Macheso's management has finally release...</td>\n",
       "      <td>https://www.herald.co.zw/macheso-names-kupa-ku...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Pakare Paye Product Relishes Tuku Festival</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>SEASONED arts consultant and publicist, Eugine...</td>\n",
       "      <td>https://www.herald.co.zw/pakare-paye-product-r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>78 CCC Activists Want Charges Quashed</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>Lawyers representing the 78 Citizens Coalition...</td>\n",
       "      <td>https://www.herald.co.zw/78-ccc-activists-want...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Mnangagwa Taking Over SADC Chairmanship a Trav...</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>ZIMBABWEANS living abroad have raised concern ...</td>\n",
       "      <td>https://www.newzimbabwe.com/mnangagwa-taking-o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Police Crackdown On Vendors, Touts, Car Wash O...</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>The Zimbabwe Republic Police (ZRP) in conjunct...</td>\n",
       "      <td>https://www.newzimbabwe.com/police-crackdown-o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Mutapa Fund Injects U.S.$5 Million Into Invict...</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>MUTAPA Investment Fund (MIF) has contributed U...</td>\n",
       "      <td>https://www.newzimbabwe.com/mutapa-fund-inject...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                TITLE   COUNTRY  \\\n",
       "0   Zimbabwean in South Africa Challenges Exorbita...  zimbabwe   \n",
       "1   American Star Tiffany Haddish Gives Zimbabwe a...  zimbabwe   \n",
       "2   Insect Farming New Frontier for Smallholder Li...  zimbabwe   \n",
       "3                    Zanu-PF, Swapo in Key Engagement  zimbabwe   \n",
       "4                                                 NaN  zimbabwe   \n",
       "5   Zanu-PF Official in Court Over $4 Million Frau...  zimbabwe   \n",
       "6   Despite Hostility, LGBTQI+ Activists in Zimbab...  zimbabwe   \n",
       "7         Zim Swimmer Weisthuizen Out of the Olympics  zimbabwe   \n",
       "8                          Dynamos' Mangombe in a Fix  zimbabwe   \n",
       "9             UN Tourism Delegation Visits First Lady  zimbabwe   \n",
       "10            VP Chiwenga Rallies SADC to Boost Trade  zimbabwe   \n",
       "11      VP Mohadi in Iran for Pezeshkian Inauguration  zimbabwe   \n",
       "12                          Wheat Clusters Take Shape  zimbabwe   \n",
       "13                              Tyla Captivates Crowd  zimbabwe   \n",
       "14              Tourism Receipts Up 35pc to U.S.$241m  zimbabwe   \n",
       "15              Air Zim Resumes Harare-Joburg Flights  zimbabwe   \n",
       "16  Air Ambulances Training Starts. . .300 Special...  zimbabwe   \n",
       "17  AfCFTA Pursues Exports for Africa's Food Security  zimbabwe   \n",
       "18          The Essential Role of Liability Insurance  zimbabwe   \n",
       "19                    Sables Lift Up Nation's Spirits  zimbabwe   \n",
       "20       Afreximbank Earmarks U.S.$400m for Zim Firms  zimbabwe   \n",
       "21  Presidential Solar Scheme Innovative, Imaginative  zimbabwe   \n",
       "22  US Diplomat Runs Over 11-Year-Old Girl, Flees ...  zimbabwe   \n",
       "23                         Police Hunt for 8 Suspects  zimbabwe   \n",
       "24                                Adachi Finding Form  zimbabwe   \n",
       "25  'Scrapping Vat On Cattle Sales Will Unlock Tru...  zimbabwe   \n",
       "26           ZRP Bans Dangerous Weapons in Beitbridge  zimbabwe   \n",
       "27      Macheso Names 'Kupa Kuturika' Album Tracklist  zimbabwe   \n",
       "28         Pakare Paye Product Relishes Tuku Festival  zimbabwe   \n",
       "29              78 CCC Activists Want Charges Quashed  zimbabwe   \n",
       "30  Mnangagwa Taking Over SADC Chairmanship a Trav...  zimbabwe   \n",
       "31  Police Crackdown On Vendors, Touts, Car Wash O...  zimbabwe   \n",
       "32  Mutapa Fund Injects U.S.$5 Million Into Invict...  zimbabwe   \n",
       "\n",
       "                                                 BODY  \\\n",
       "0   A Zimbabwean resident in South Africa, Takudzw...   \n",
       "1   VISITING American comedienne, Tiffany Haddish,...   \n",
       "2   Zimbabwe, just like many other countries, is l...   \n",
       "3   ZANU PF and SWAPO of Namibia share common bond...   \n",
       "4                                                 NaN   \n",
       "5   A senior official from Zimbabwe's ruling Zanu ...   \n",
       "6   Munich, Germany —  LGBTQI+ people have long be...   \n",
       "7   ZIMBABWE swimmer Paige Van Der Weisthuizen was...   \n",
       "8   Castle Lager Premiership giants, Dynamos head ...   \n",
       "9   Victoria Falls — United Nations Tourism Secret...   \n",
       "10  SADC countries must use their regional advanta...   \n",
       "11  Tehran, Iran — Vice President Kembo Mohadi has...   \n",
       "12  CLUSTERING of wheat farmers for easy service d...   \n",
       "13  Johannesburg. — Tyla captivated the Paris crow...   \n",
       "14  Zimbabwe's tourism receipts grew by an estimat...   \n",
       "15  Air Zimbabwe yesterday resumed flights from Ha...   \n",
       "16  THE Russian HeliDrive Air ambulances medical s...   \n",
       "17  THE African Continental Free Trade Area (AfCFT...   \n",
       "18  Liability insurance is a critical safeguard fo...   \n",
       "19  After breaking a couple of jinxes to be crowne...   \n",
       "20  The African Export-Import Bank (Afreximbank) h...   \n",
       "21  This is an installation by Zimbabwe Solar Ener...   \n",
       "22  A United States Embassy official allegedly ran...   \n",
       "23  POLICE in Harare have launched a manhunt for e...   \n",
       "24  Adachi, who had a poor start to the Zifa South...   \n",
       "25  Stakeholders in the beef industry have describ...   \n",
       "26  The Zimbabwe Republic Police (ZRP) has issued ...   \n",
       "27  ALICK Macheso's management has finally release...   \n",
       "28  SEASONED arts consultant and publicist, Eugine...   \n",
       "29  Lawyers representing the 78 Citizens Coalition...   \n",
       "30  ZIMBABWEANS living abroad have raised concern ...   \n",
       "31  The Zimbabwe Republic Police (ZRP) in conjunct...   \n",
       "32  MUTAPA Investment Fund (MIF) has contributed U...   \n",
       "\n",
       "                                         Website Link  \n",
       "0   https://www.newzimbabwe.com/zimbabwean-in-sout...  \n",
       "1   https://www.herald.co.zw/american-star-tiffany...  \n",
       "2   https://www.herald.co.zw/insect-farming-new-fr...  \n",
       "3   https://www.herald.co.zw/zanu-pf-swapo-in-key-...  \n",
       "4                                                 NaN  \n",
       "5   https://www.263chat.com/zanu-pf-official-in-co...  \n",
       "6                                                 NaN  \n",
       "7   https://www.newzimbabwe.com/zim-swimmer-weisth...  \n",
       "8   https://www.herald.co.zw/dynamos-mangombe-in-a...  \n",
       "9   https://www.herald.co.zw/un-tourism-delegation...  \n",
       "10  https://www.herald.co.zw/vp-chiwenga-rallies-s...  \n",
       "11  https://www.herald.co.zw/vp-mohadi-in-iran-for...  \n",
       "12  https://www.herald.co.zw/wheat-clusters-take-s...  \n",
       "13    https://www.herald.co.zw/tyla-captivates-crowd/  \n",
       "14  https://www.herald.co.zw/tourism-receipts-up-3...  \n",
       "15  https://www.herald.co.zw/air-zim-resumes-harar...  \n",
       "16  https://www.herald.co.zw/air-ambulances-traini...  \n",
       "17  https://www.herald.co.zw/afcfta-pursues-export...  \n",
       "18  https://www.herald.co.zw/the-essential-role-of...  \n",
       "19  https://www.herald.co.zw/sables-lift-up-nation...  \n",
       "20  https://www.herald.co.zw/afreximbank-earmarks-...  \n",
       "21  https://www.herald.co.zw/editorial-comment-pre...  \n",
       "22  https://www.herald.co.zw/us-diplomat-runs-over...  \n",
       "23  https://www.herald.co.zw/police-hunt-for-8-sus...  \n",
       "24      https://www.herald.co.zw/adachi-finding-form/  \n",
       "25  https://www.herald.co.zw/scrapping-vat-on-catt...  \n",
       "26  https://www.herald.co.zw/zrp-bans-dangerous-we...  \n",
       "27  https://www.herald.co.zw/macheso-names-kupa-ku...  \n",
       "28  https://www.herald.co.zw/pakare-paye-product-r...  \n",
       "29  https://www.herald.co.zw/78-ccc-activists-want...  \n",
       "30  https://www.newzimbabwe.com/mnangagwa-taking-o...  \n",
       "31  https://www.newzimbabwe.com/police-crackdown-o...  \n",
       "32  https://www.newzimbabwe.com/mutapa-fund-inject...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if upload_completed:\n",
    "    newsdata = pd.read_csv(file_path)\n",
    "    display(newsdata)\n",
    "else:\n",
    "    print(\"Upload a file first and then proceed to load the data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "vw1ingYr9ZLQ"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>COUNTRY</th>\n",
       "      <th>BODY</th>\n",
       "      <th>Website Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Zimbabwean in South Africa Challenges Exorbita...</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>A Zimbabwean resident in South Africa, Takudzw...</td>\n",
       "      <td>https://www.newzimbabwe.com/zimbabwean-in-sout...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>American Star Tiffany Haddish Gives Zimbabwe a...</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>VISITING American comedienne, Tiffany Haddish,...</td>\n",
       "      <td>https://www.herald.co.zw/american-star-tiffany...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Insect Farming New Frontier for Smallholder Li...</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>Zimbabwe, just like many other countries, is l...</td>\n",
       "      <td>https://www.herald.co.zw/insect-farming-new-fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Zanu-PF, Swapo in Key Engagement</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>ZANU PF and SWAPO of Namibia share common bond...</td>\n",
       "      <td>https://www.herald.co.zw/zanu-pf-swapo-in-key-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Zanu-PF Official in Court Over $4 Million Frau...</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>A senior official from Zimbabwe's ruling Zanu ...</td>\n",
       "      <td>https://www.263chat.com/zanu-pf-official-in-co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Despite Hostility, LGBTQI+ Activists in Zimbab...</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>Munich, Germany —  LGBTQI+ people have long be...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Zim Swimmer Weisthuizen Out of the Olympics</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>ZIMBABWE swimmer Paige Van Der Weisthuizen was...</td>\n",
       "      <td>https://www.newzimbabwe.com/zim-swimmer-weisth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Dynamos' Mangombe in a Fix</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>Castle Lager Premiership giants, Dynamos head ...</td>\n",
       "      <td>https://www.herald.co.zw/dynamos-mangombe-in-a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>UN Tourism Delegation Visits First Lady</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>Victoria Falls — United Nations Tourism Secret...</td>\n",
       "      <td>https://www.herald.co.zw/un-tourism-delegation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>VP Chiwenga Rallies SADC to Boost Trade</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>SADC countries must use their regional advanta...</td>\n",
       "      <td>https://www.herald.co.zw/vp-chiwenga-rallies-s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>VP Mohadi in Iran for Pezeshkian Inauguration</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>Tehran, Iran — Vice President Kembo Mohadi has...</td>\n",
       "      <td>https://www.herald.co.zw/vp-mohadi-in-iran-for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Wheat Clusters Take Shape</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>CLUSTERING of wheat farmers for easy service d...</td>\n",
       "      <td>https://www.herald.co.zw/wheat-clusters-take-s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Tyla Captivates Crowd</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>Johannesburg. — Tyla captivated the Paris crow...</td>\n",
       "      <td>https://www.herald.co.zw/tyla-captivates-crowd/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Tourism Receipts Up 35pc to U.S.$241m</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>Zimbabwe's tourism receipts grew by an estimat...</td>\n",
       "      <td>https://www.herald.co.zw/tourism-receipts-up-3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Air Zim Resumes Harare-Joburg Flights</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>Air Zimbabwe yesterday resumed flights from Ha...</td>\n",
       "      <td>https://www.herald.co.zw/air-zim-resumes-harar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Air Ambulances Training Starts. . .300 Special...</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>THE Russian HeliDrive Air ambulances medical s...</td>\n",
       "      <td>https://www.herald.co.zw/air-ambulances-traini...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>AfCFTA Pursues Exports for Africa's Food Security</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>THE African Continental Free Trade Area (AfCFT...</td>\n",
       "      <td>https://www.herald.co.zw/afcfta-pursues-export...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>The Essential Role of Liability Insurance</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>Liability insurance is a critical safeguard fo...</td>\n",
       "      <td>https://www.herald.co.zw/the-essential-role-of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Sables Lift Up Nation's Spirits</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>After breaking a couple of jinxes to be crowne...</td>\n",
       "      <td>https://www.herald.co.zw/sables-lift-up-nation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Afreximbank Earmarks U.S.$400m for Zim Firms</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>The African Export-Import Bank (Afreximbank) h...</td>\n",
       "      <td>https://www.herald.co.zw/afreximbank-earmarks-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Presidential Solar Scheme Innovative, Imaginative</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>This is an installation by Zimbabwe Solar Ener...</td>\n",
       "      <td>https://www.herald.co.zw/editorial-comment-pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>US Diplomat Runs Over 11-Year-Old Girl, Flees ...</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>A United States Embassy official allegedly ran...</td>\n",
       "      <td>https://www.herald.co.zw/us-diplomat-runs-over...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Police Hunt for 8 Suspects</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>POLICE in Harare have launched a manhunt for e...</td>\n",
       "      <td>https://www.herald.co.zw/police-hunt-for-8-sus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Adachi Finding Form</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>Adachi, who had a poor start to the Zifa South...</td>\n",
       "      <td>https://www.herald.co.zw/adachi-finding-form/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>'Scrapping Vat On Cattle Sales Will Unlock Tru...</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>Stakeholders in the beef industry have describ...</td>\n",
       "      <td>https://www.herald.co.zw/scrapping-vat-on-catt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ZRP Bans Dangerous Weapons in Beitbridge</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>The Zimbabwe Republic Police (ZRP) has issued ...</td>\n",
       "      <td>https://www.herald.co.zw/zrp-bans-dangerous-we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Macheso Names 'Kupa Kuturika' Album Tracklist</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>ALICK Macheso's management has finally release...</td>\n",
       "      <td>https://www.herald.co.zw/macheso-names-kupa-ku...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Pakare Paye Product Relishes Tuku Festival</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>SEASONED arts consultant and publicist, Eugine...</td>\n",
       "      <td>https://www.herald.co.zw/pakare-paye-product-r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>78 CCC Activists Want Charges Quashed</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>Lawyers representing the 78 Citizens Coalition...</td>\n",
       "      <td>https://www.herald.co.zw/78-ccc-activists-want...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Mnangagwa Taking Over SADC Chairmanship a Trav...</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>ZIMBABWEANS living abroad have raised concern ...</td>\n",
       "      <td>https://www.newzimbabwe.com/mnangagwa-taking-o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Police Crackdown On Vendors, Touts, Car Wash O...</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>The Zimbabwe Republic Police (ZRP) in conjunct...</td>\n",
       "      <td>https://www.newzimbabwe.com/police-crackdown-o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Mutapa Fund Injects U.S.$5 Million Into Invict...</td>\n",
       "      <td>zimbabwe</td>\n",
       "      <td>MUTAPA Investment Fund (MIF) has contributed U...</td>\n",
       "      <td>https://www.newzimbabwe.com/mutapa-fund-inject...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                TITLE   COUNTRY  \\\n",
       "0   Zimbabwean in South Africa Challenges Exorbita...  zimbabwe   \n",
       "1   American Star Tiffany Haddish Gives Zimbabwe a...  zimbabwe   \n",
       "2   Insect Farming New Frontier for Smallholder Li...  zimbabwe   \n",
       "3                    Zanu-PF, Swapo in Key Engagement  zimbabwe   \n",
       "4                                                 NaN  zimbabwe   \n",
       "5   Zanu-PF Official in Court Over $4 Million Frau...  zimbabwe   \n",
       "6   Despite Hostility, LGBTQI+ Activists in Zimbab...  zimbabwe   \n",
       "7         Zim Swimmer Weisthuizen Out of the Olympics  zimbabwe   \n",
       "8                          Dynamos' Mangombe in a Fix  zimbabwe   \n",
       "9             UN Tourism Delegation Visits First Lady  zimbabwe   \n",
       "10            VP Chiwenga Rallies SADC to Boost Trade  zimbabwe   \n",
       "11      VP Mohadi in Iran for Pezeshkian Inauguration  zimbabwe   \n",
       "12                          Wheat Clusters Take Shape  zimbabwe   \n",
       "13                              Tyla Captivates Crowd  zimbabwe   \n",
       "14              Tourism Receipts Up 35pc to U.S.$241m  zimbabwe   \n",
       "15              Air Zim Resumes Harare-Joburg Flights  zimbabwe   \n",
       "16  Air Ambulances Training Starts. . .300 Special...  zimbabwe   \n",
       "17  AfCFTA Pursues Exports for Africa's Food Security  zimbabwe   \n",
       "18          The Essential Role of Liability Insurance  zimbabwe   \n",
       "19                    Sables Lift Up Nation's Spirits  zimbabwe   \n",
       "20       Afreximbank Earmarks U.S.$400m for Zim Firms  zimbabwe   \n",
       "21  Presidential Solar Scheme Innovative, Imaginative  zimbabwe   \n",
       "22  US Diplomat Runs Over 11-Year-Old Girl, Flees ...  zimbabwe   \n",
       "23                         Police Hunt for 8 Suspects  zimbabwe   \n",
       "24                                Adachi Finding Form  zimbabwe   \n",
       "25  'Scrapping Vat On Cattle Sales Will Unlock Tru...  zimbabwe   \n",
       "26           ZRP Bans Dangerous Weapons in Beitbridge  zimbabwe   \n",
       "27      Macheso Names 'Kupa Kuturika' Album Tracklist  zimbabwe   \n",
       "28         Pakare Paye Product Relishes Tuku Festival  zimbabwe   \n",
       "29              78 CCC Activists Want Charges Quashed  zimbabwe   \n",
       "30  Mnangagwa Taking Over SADC Chairmanship a Trav...  zimbabwe   \n",
       "31  Police Crackdown On Vendors, Touts, Car Wash O...  zimbabwe   \n",
       "32  Mutapa Fund Injects U.S.$5 Million Into Invict...  zimbabwe   \n",
       "\n",
       "                                                 BODY  \\\n",
       "0   A Zimbabwean resident in South Africa, Takudzw...   \n",
       "1   VISITING American comedienne, Tiffany Haddish,...   \n",
       "2   Zimbabwe, just like many other countries, is l...   \n",
       "3   ZANU PF and SWAPO of Namibia share common bond...   \n",
       "4                                                 NaN   \n",
       "5   A senior official from Zimbabwe's ruling Zanu ...   \n",
       "6   Munich, Germany —  LGBTQI+ people have long be...   \n",
       "7   ZIMBABWE swimmer Paige Van Der Weisthuizen was...   \n",
       "8   Castle Lager Premiership giants, Dynamos head ...   \n",
       "9   Victoria Falls — United Nations Tourism Secret...   \n",
       "10  SADC countries must use their regional advanta...   \n",
       "11  Tehran, Iran — Vice President Kembo Mohadi has...   \n",
       "12  CLUSTERING of wheat farmers for easy service d...   \n",
       "13  Johannesburg. — Tyla captivated the Paris crow...   \n",
       "14  Zimbabwe's tourism receipts grew by an estimat...   \n",
       "15  Air Zimbabwe yesterday resumed flights from Ha...   \n",
       "16  THE Russian HeliDrive Air ambulances medical s...   \n",
       "17  THE African Continental Free Trade Area (AfCFT...   \n",
       "18  Liability insurance is a critical safeguard fo...   \n",
       "19  After breaking a couple of jinxes to be crowne...   \n",
       "20  The African Export-Import Bank (Afreximbank) h...   \n",
       "21  This is an installation by Zimbabwe Solar Ener...   \n",
       "22  A United States Embassy official allegedly ran...   \n",
       "23  POLICE in Harare have launched a manhunt for e...   \n",
       "24  Adachi, who had a poor start to the Zifa South...   \n",
       "25  Stakeholders in the beef industry have describ...   \n",
       "26  The Zimbabwe Republic Police (ZRP) has issued ...   \n",
       "27  ALICK Macheso's management has finally release...   \n",
       "28  SEASONED arts consultant and publicist, Eugine...   \n",
       "29  Lawyers representing the 78 Citizens Coalition...   \n",
       "30  ZIMBABWEANS living abroad have raised concern ...   \n",
       "31  The Zimbabwe Republic Police (ZRP) in conjunct...   \n",
       "32  MUTAPA Investment Fund (MIF) has contributed U...   \n",
       "\n",
       "                                         Website Link  \n",
       "0   https://www.newzimbabwe.com/zimbabwean-in-sout...  \n",
       "1   https://www.herald.co.zw/american-star-tiffany...  \n",
       "2   https://www.herald.co.zw/insect-farming-new-fr...  \n",
       "3   https://www.herald.co.zw/zanu-pf-swapo-in-key-...  \n",
       "4                                                 NaN  \n",
       "5   https://www.263chat.com/zanu-pf-official-in-co...  \n",
       "6                                                 NaN  \n",
       "7   https://www.newzimbabwe.com/zim-swimmer-weisth...  \n",
       "8   https://www.herald.co.zw/dynamos-mangombe-in-a...  \n",
       "9   https://www.herald.co.zw/un-tourism-delegation...  \n",
       "10  https://www.herald.co.zw/vp-chiwenga-rallies-s...  \n",
       "11  https://www.herald.co.zw/vp-mohadi-in-iran-for...  \n",
       "12  https://www.herald.co.zw/wheat-clusters-take-s...  \n",
       "13    https://www.herald.co.zw/tyla-captivates-crowd/  \n",
       "14  https://www.herald.co.zw/tourism-receipts-up-3...  \n",
       "15  https://www.herald.co.zw/air-zim-resumes-harar...  \n",
       "16  https://www.herald.co.zw/air-ambulances-traini...  \n",
       "17  https://www.herald.co.zw/afcfta-pursues-export...  \n",
       "18  https://www.herald.co.zw/the-essential-role-of...  \n",
       "19  https://www.herald.co.zw/sables-lift-up-nation...  \n",
       "20  https://www.herald.co.zw/afreximbank-earmarks-...  \n",
       "21  https://www.herald.co.zw/editorial-comment-pre...  \n",
       "22  https://www.herald.co.zw/us-diplomat-runs-over...  \n",
       "23  https://www.herald.co.zw/police-hunt-for-8-sus...  \n",
       "24      https://www.herald.co.zw/adachi-finding-form/  \n",
       "25  https://www.herald.co.zw/scrapping-vat-on-catt...  \n",
       "26  https://www.herald.co.zw/zrp-bans-dangerous-we...  \n",
       "27  https://www.herald.co.zw/macheso-names-kupa-ku...  \n",
       "28  https://www.herald.co.zw/pakare-paye-product-r...  \n",
       "29  https://www.herald.co.zw/78-ccc-activists-want...  \n",
       "30  https://www.newzimbabwe.com/mnangagwa-taking-o...  \n",
       "31  https://www.newzimbabwe.com/police-crackdown-o...  \n",
       "32  https://www.newzimbabwe.com/mutapa-fund-inject...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "D_nXuAVy_Dxn"
   },
   "outputs": [],
   "source": [
    "newsdata = newsdata.dropna()\n",
    "newsdata_body = newsdata['BODY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ulkF1g9__Ixt"
   },
   "source": [
    "**Function to split text into sentences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ulkF1g9__Ixt"
   },
   "outputs": [],
   "source": [
    "def split_text_by_sentences(text):\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocess the data by converting it to lower case** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "EisTeccSRtv8"
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()  # Lowercase\n",
    "    # Add more preprocessing steps if needed\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "UQvUzmRjdTch"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1d/qzlhmy690tvcd1wf5tpyr_km0000gn/T/ipykernel_13991/3866009567.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  newsdata.loc[:, 'cleaned_body'] = newsdata['BODY'].apply(preprocess)\n"
     ]
    }
   ],
   "source": [
    "newsdata.loc[:, 'cleaned_body'] = newsdata['BODY'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use TF-IDF to retrieve most important terms in the news data and select top 10 news articles\n",
    "The TFidVectorizer is sued to transform the body of the articles into numeric form, finding the top term based on their term frequencies. These terms aare then counted in each article, where the articles are arranged based on highest number of terms present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZnWHJTFodXNE"
   },
   "source": [
    "**Vectorize the text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ZnWHJTFodXNE"
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=list(ENGLISH_STOP_WORDS), max_features=1000)\n",
    "X = vectorizer.fit_transform(newsdata['cleaned_body'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZnWHJTFodXNE"
   },
   "source": [
    "**Calculate term frequencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ZnWHJTFodXNE"
   },
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "tfidf_scores = X.sum(axis=0).A1\n",
    "term_scores = dict(zip(feature_names, tfidf_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZnWHJTFodXNE"
   },
   "source": [
    "**Get top 10 trending topics based on TF-IDF scores using the 20 most common terms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "ZnWHJTFodXNE"
   },
   "outputs": [],
   "source": [
    "top_terms = Counter(term_scores).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Rl_SFn_Ydfa1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Relevant Terms:\n",
      "said: 1.617239418765897\n",
      "sadc: 1.5866304053154328\n",
      "zimbabwe: 1.5387162220256676\n",
      "africa: 1.0980387025610683\n",
      "investment: 1.0527951933568427\n",
      "tourism: 1.0083783864340194\n",
      "harare: 0.9963720158183158\n",
      "country: 0.9404634824655383\n",
      "police: 0.9276228224586293\n",
      "million: 0.9266161434507789\n",
      "region: 0.9027383259263309\n",
      "season: 0.8665253037480395\n",
      "mr: 0.8141305710770922\n",
      "president: 0.7944616990579325\n",
      "government: 0.742501399340561\n",
      "year: 0.7418823191337467\n",
      "percent: 0.7307642736221417\n",
      "international: 0.7276504583290118\n",
      "mnangagwa: 0.7108881419472728\n",
      "economic: 0.707578804741229\n"
     ]
    }
   ],
   "source": [
    "print(\"Top Relevant Terms:\")\n",
    "for term, score in top_terms:\n",
    "    print(f\"{term}: {score}\")\n",
    "\n",
    "article_scores = []\n",
    "\n",
    "# Calculate relevance score for each article based on top terms\n",
    "for index, row in newsdata.iterrows():\n",
    "    relevance_score = 0\n",
    "    for term, _ in top_terms:\n",
    "        relevance_score += row['cleaned_body'].count(term)  # Count occurrences of the term\n",
    "    article_scores.append({'TITLE': row['TITLE'],\n",
    "                           'URL': row['Website Link'],\n",
    "                           'BODY':row['BODY'],\n",
    "                           'Relevance': relevance_score,\n",
    "                           })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "sUA1z4zfdmuK"
   },
   "outputs": [],
   "source": [
    "# Convert list to DataFrame\n",
    "article_scores_df = pd.DataFrame(article_scores)\n",
    "\n",
    "# Sort articles by relevance score and get top 10\n",
    "top_10_articles = article_scores_df.sort_values(by='Relevance', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "YDIwo1xnAOYU"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>URL</th>\n",
       "      <th>BODY</th>\n",
       "      <th>Relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Afreximbank Earmarks U.S.$400m for Zim Firms</td>\n",
       "      <td>https://www.herald.co.zw/afreximbank-earmarks-...</td>\n",
       "      <td>The African Export-Import Bank (Afreximbank) h...</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>VP Chiwenga Rallies SADC to Boost Trade</td>\n",
       "      <td>https://www.herald.co.zw/vp-chiwenga-rallies-s...</td>\n",
       "      <td>SADC countries must use their regional advanta...</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Mnangagwa Taking Over SADC Chairmanship a Trav...</td>\n",
       "      <td>https://www.newzimbabwe.com/mnangagwa-taking-o...</td>\n",
       "      <td>ZIMBABWEANS living abroad have raised concern ...</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Tourism Receipts Up 35pc to U.S.$241m</td>\n",
       "      <td>https://www.herald.co.zw/tourism-receipts-up-3...</td>\n",
       "      <td>Zimbabwe's tourism receipts grew by an estimat...</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>US Diplomat Runs Over 11-Year-Old Girl, Flees ...</td>\n",
       "      <td>https://www.herald.co.zw/us-diplomat-runs-over...</td>\n",
       "      <td>A United States Embassy official allegedly ran...</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>AfCFTA Pursues Exports for Africa's Food Security</td>\n",
       "      <td>https://www.herald.co.zw/afcfta-pursues-export...</td>\n",
       "      <td>THE African Continental Free Trade Area (AfCFT...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>UN Tourism Delegation Visits First Lady</td>\n",
       "      <td>https://www.herald.co.zw/un-tourism-delegation...</td>\n",
       "      <td>Victoria Falls — United Nations Tourism Secret...</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Air Ambulances Training Starts. . .300 Special...</td>\n",
       "      <td>https://www.herald.co.zw/air-ambulances-traini...</td>\n",
       "      <td>THE Russian HeliDrive Air ambulances medical s...</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Air Zim Resumes Harare-Joburg Flights</td>\n",
       "      <td>https://www.herald.co.zw/air-zim-resumes-harar...</td>\n",
       "      <td>Air Zimbabwe yesterday resumed flights from Ha...</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Police Crackdown On Vendors, Touts, Car Wash O...</td>\n",
       "      <td>https://www.newzimbabwe.com/police-crackdown-o...</td>\n",
       "      <td>The Zimbabwe Republic Police (ZRP) in conjunct...</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                TITLE  \\\n",
       "18       Afreximbank Earmarks U.S.$400m for Zim Firms   \n",
       "8             VP Chiwenga Rallies SADC to Boost Trade   \n",
       "28  Mnangagwa Taking Over SADC Chairmanship a Trav...   \n",
       "12              Tourism Receipts Up 35pc to U.S.$241m   \n",
       "20  US Diplomat Runs Over 11-Year-Old Girl, Flees ...   \n",
       "15  AfCFTA Pursues Exports for Africa's Food Security   \n",
       "7             UN Tourism Delegation Visits First Lady   \n",
       "14  Air Ambulances Training Starts. . .300 Special...   \n",
       "13              Air Zim Resumes Harare-Joburg Flights   \n",
       "29  Police Crackdown On Vendors, Touts, Car Wash O...   \n",
       "\n",
       "                                                  URL  \\\n",
       "18  https://www.herald.co.zw/afreximbank-earmarks-...   \n",
       "8   https://www.herald.co.zw/vp-chiwenga-rallies-s...   \n",
       "28  https://www.newzimbabwe.com/mnangagwa-taking-o...   \n",
       "12  https://www.herald.co.zw/tourism-receipts-up-3...   \n",
       "20  https://www.herald.co.zw/us-diplomat-runs-over...   \n",
       "15  https://www.herald.co.zw/afcfta-pursues-export...   \n",
       "7   https://www.herald.co.zw/un-tourism-delegation...   \n",
       "14  https://www.herald.co.zw/air-ambulances-traini...   \n",
       "13  https://www.herald.co.zw/air-zim-resumes-harar...   \n",
       "29  https://www.newzimbabwe.com/police-crackdown-o...   \n",
       "\n",
       "                                                 BODY  Relevance  \n",
       "18  The African Export-Import Bank (Afreximbank) h...        136  \n",
       "8   SADC countries must use their regional advanta...         77  \n",
       "28  ZIMBABWEANS living abroad have raised concern ...         75  \n",
       "12  Zimbabwe's tourism receipts grew by an estimat...         63  \n",
       "20  A United States Embassy official allegedly ran...         52  \n",
       "15  THE African Continental Free Trade Area (AfCFT...         49  \n",
       "7   Victoria Falls — United Nations Tourism Secret...         48  \n",
       "14  THE Russian HeliDrive Air ambulances medical s...         44  \n",
       "13  Air Zimbabwe yesterday resumed flights from Ha...         32  \n",
       "29  The Zimbabwe Republic Police (ZRP) in conjunct...         28  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_10_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1a765bda"
   },
   "source": [
    "### Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3           \n",
    "Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description:** Apply the distilBERT base uncased SST-2 sentiment analysis tool from using Hugging Face, to\n",
    "understand and display the emotional tone of news articles, either negative or positive.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About distilBERT\n",
    "The distilbert-base-uncased-finetuned-sst-2-english is applied on the dataset(more specifically the body of the articles to acquire the sentiment analysis of the article in question. This is done by splitting the articles into chunks and passing this chunks into the model, the sentiment scores of all these are articles are averaged and the average score of either negative or positive is compared, The highest average is then selected, then displaying the sentiment analysis as either Positive or negative. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "NU1lriBuAkl7",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TITLE OF ARTICLE: Afreximbank Earmarks U.S.$400m for Zim Firms\n",
      "Overall Sentiment: POSITIVE\n",
      "Average Sentiment Score: 0.9633\n",
      "\n",
      "TITLE OF ARTICLE: VP Chiwenga Rallies SADC to Boost Trade\n",
      "Overall Sentiment: POSITIVE\n",
      "Average Sentiment Score: 0.9920\n",
      "\n",
      "TITLE OF ARTICLE: Mnangagwa Taking Over SADC Chairmanship a Travesty - Diasporans Accuse Regional Bloc of Legitimizing Tyranny\n",
      "Overall Sentiment: NEGATIVE\n",
      "Average Sentiment Score: 0.9585\n",
      "\n",
      "TITLE OF ARTICLE: Tourism Receipts Up 35pc to U.S.$241m\n",
      "Overall Sentiment: POSITIVE\n",
      "Average Sentiment Score: 0.9916\n",
      "\n",
      "TITLE OF ARTICLE: US Diplomat Runs Over 11-Year-Old Girl, Flees Country\n",
      "Overall Sentiment: NEGATIVE\n",
      "Average Sentiment Score: 0.9706\n",
      "\n",
      "TITLE OF ARTICLE: AfCFTA Pursues Exports for Africa's Food Security\n",
      "Overall Sentiment: POSITIVE\n",
      "Average Sentiment Score: 0.9948\n",
      "\n",
      "TITLE OF ARTICLE: UN Tourism Delegation Visits First Lady\n",
      "Overall Sentiment: NEGATIVE\n",
      "Average Sentiment Score: 0.9741\n",
      "\n",
      "TITLE OF ARTICLE: Air Ambulances Training Starts. . .300 Specialised Jobs On Cards\n",
      "Overall Sentiment: NEGATIVE\n",
      "Average Sentiment Score: 0.9231\n",
      "\n",
      "TITLE OF ARTICLE: Air Zim Resumes Harare-Joburg Flights\n",
      "Overall Sentiment: NEGATIVE\n",
      "Average Sentiment Score: 0.9948\n",
      "\n",
      "TITLE OF ARTICLE: Police Crackdown On Vendors, Touts, Car Wash Operators Ahead of SADC Summit\n",
      "Overall Sentiment: NEGATIVE\n",
      "Average Sentiment Score: 0.9763\n"
     ]
    }
   ],
   "source": [
    "for text in top_10_articles['BODY']:\n",
    "    sentences = split_text_by_sentences(text)\n",
    "    results = []\n",
    "\n",
    "    # Analyze each sentence and calculate the average sentiment score\n",
    "    for sentence in sentences:\n",
    "        result = sentiment_analyzer(sentence)[0]\n",
    "        results.append(result)\n",
    "        #print(f\"Sentence: {sentence}\\nSentiment: {result['label']}, Probability: {result['score']:.4f}\\n\")\n",
    " \n",
    "    # Calculate the average sentiment score\n",
    "    positive_scores = [result['score'] for result in results if result['label'] == 'POSITIVE']\n",
    "    negative_scores = [result['score'] for result in results if result['label'] == 'NEGATIVE']\n",
    "\n",
    "    average_positive = sum(positive_scores) / len(positive_scores) if positive_scores else 0\n",
    "    average_negative = sum(negative_scores) / len(negative_scores) if negative_scores else 0\n",
    "\n",
    "    # Determine the overall average sentiment\n",
    "    overall_sentiment = \"POSITIVE\" if average_positive >= average_negative else \"NEGATIVE\"\n",
    "    average_score = max(average_positive, average_negative)\n",
    "\n",
    "    for index, row in top_10_articles.iterrows():\n",
    "        if text in row['BODY']:\n",
    "            print()\n",
    "            print(\"TITLE OF ARTICLE: \"+ row['TITLE'])\n",
    "\n",
    "    print(f\"Overall Sentiment: {overall_sentiment}\")\n",
    "    print(f\"Average Sentiment Score: {average_score:.4f}\")\n",
    "\n",
    "    '''# Display the probabilities for each sentence\n",
    "    for i, result in enumerate(results):\n",
    "      print(f\"Sentence {i+1} Sentiment: {result['label']}, Probability: {result['score']:.4f}\")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1a765bda"
   },
   "source": [
    "### Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2DeoMQnnfCr"
   },
   "source": [
    "# Phase 4         \n",
    "Summarize News Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A9jXrb4BnfCs"
   },
   "source": [
    "This notebook addresses Phase 4 of the What's The Hot Topic in Topic Town? project: **Summarize News Articles**.  \n",
    "The self-created rubric, in our repository, explains the requirement for a proper execution of this phase as seen below.   \n",
    "**Description:** Fine tune the BERT base cased model from Hugging Face with labeled dataset of news articles\n",
    "and generate one paragraph news summaries with it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wA3pjxAnfCt"
   },
   "source": [
    "### Repository Link\n",
    "\n",
    "Here is a link to our repository:\n",
    "\n",
    "[What's The Hot Topic In Town?](https://github.com/kelvin-ahiakpor/Whats.The.Hot.Topic.In.Town)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "de02dd80"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-02T14:41:22.913248Z",
     "iopub.status.busy": "2024-08-02T14:41:22.912735Z",
     "iopub.status.idle": "2024-08-02T14:42:42.089722Z",
     "shell.execute_reply": "2024-08-02T14:42:42.088757Z",
     "shell.execute_reply.started": "2024-08-02T14:41:22.913213Z"
    },
    "id": "w67vkz3KP9eZ"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install datasets==1.0.2\n",
    "!pip install transformers==4.2.1\n",
    "!pip install torch evaluate accelerate pandas rouge_score wandb sacrebleu wandb\n",
    "\n",
    "import os\n",
    "import wandb\n",
    "import torch\n",
    "import shutil\n",
    "import evaluate\n",
    "import datasets\n",
    "import accelerate\n",
    "import transformers\n",
    "\n",
    "import pandas as pd\n",
    "from torch.cuda.amp import autocast\n",
    "from IPython.display import display, HTML\n",
    "from datasets import ClassLabel\n",
    "from datasets import load_metric\n",
    "\n",
    "from transformers import pipeline\n",
    "from transformers import BertTokenizerFast\n",
    "from transformers import EncoderDecoderModel\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0001aed4"
   },
   "source": [
    "### Setting job timeout for computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-02T14:42:42.092197Z",
     "iopub.status.busy": "2024-08-02T14:42:42.091497Z",
     "iopub.status.idle": "2024-08-02T14:42:42.097255Z",
     "shell.execute_reply": "2024-08-02T14:42:42.096109Z",
     "shell.execute_reply.started": "2024-08-02T14:42:42.092166Z"
    },
    "id": "b42967b9"
   },
   "outputs": [],
   "source": [
    "os.environ['JOBLIB_START_METHOD'] = 'loky'\n",
    "os.environ['JOBLIB_TIMEOUT'] = '300'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JiVts_ArnfCx"
   },
   "source": [
    "### Setting up weights and biases (WANDB) to log training metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-08-02T14:42:42.099716Z",
     "iopub.status.busy": "2024-08-02T14:42:42.098733Z",
     "iopub.status.idle": "2024-08-02T14:42:44.209887Z",
     "shell.execute_reply": "2024-08-02T14:42:44.208866Z",
     "shell.execute_reply.started": "2024-08-02T14:42:42.099677Z"
    },
    "id": "KaGm_4tinfCx",
    "outputId": "1b41a922-e9ee-48be-aa84-20b24c999572"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkelvin-ahiakpor\u001b[0m (\u001b[33mkelvin-ahiakpor-ashesi-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['WANDB_API_KEY'] = 'removed-for-privacy'\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YsgWOquRnfCy"
   },
   "source": [
    "### Selecting first available GPU if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-02T14:42:44.212876Z",
     "iopub.status.busy": "2024-08-02T14:42:44.212165Z",
     "iopub.status.idle": "2024-08-02T14:42:44.246181Z",
     "shell.execute_reply": "2024-08-02T14:42:44.244998Z",
     "shell.execute_reply.started": "2024-08-02T14:42:44.212845Z"
    },
    "id": "Z2H98vYGnfCy"
   },
   "outputs": [],
   "source": [
    "device = 0 if torch.cuda.is_available() else -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9eqvmuKnfCy"
   },
   "source": [
    "### Loading training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 429,
     "referenced_widgets": [
      "64486fcfdaab4b9eafcb22a9043a86ca",
      "8ada612e3dd442039c9ab2cb685e7f1e",
      "40db7e611b4e4cdfb79f36826e6b01d2",
      "4e915de737ce41ca97808757c47ad967",
      "87fce1631d8b4cb0858d162f49e38d46",
      "80ba1ab244be48c4bf7a93dfb298b9e9",
      "0b1169bdd15a4b9ca76818b250361f64",
      "294a0a26deae45bd97e14b9ff391e62f",
      "0cda5ad3e6a748c694828abf40c1c44e",
      "e2c37a38283e4574a0e250d1ef5baa29",
      "e52f3d154fa3476a91eac21ae937a63a",
      "49eaf804cf4744a2a4427feca336e8e0",
      "9a4bc7bfd294477ba6e6ad3cf8df9445",
      "7cacb0c65b204b4f82a12fadd0d40be5",
      "f9b617d04a524d35b1385ba987f6e9f3",
      "5d9d50fc7bfa4fe8a1056d5cff78a948",
      "9a90f91b8eee40bca4a4539d401c7bdf",
      "77878412f89d43419aa88151339df5da",
      "e8cc86fd2bc240f39bc59dea17a38bf2",
      "6e7a2e243ce1474ab736313855b06bdc",
      "938ff2b28f4040658b27c5d0263da8cc",
      "2f5b66e36d8a4d339fc1fe5aa91a5404",
      "f15f2733215b444a8c5107da5d06a4ac",
      "5436c95f107449f4afe0cded2fcff666",
      "803de19b3a0347958077535f80ccf108",
      "eba67c720db742ebb3fe5c0a11dcf8a6",
      "f90546fc64d846f288e12ff00c7217ef",
      "8a32ee0800854179ac7116dcd1e4004b",
      "6172a70cc7904e37a20f512f32db1cd9",
      "43f6667652e147b5bef14aeb28f94967",
      "94052ed56bf947c3a6d8135ac3ba1579",
      "e39c97910d484a9e8621b67b739fbf86",
      "ead18a142f2e4b6abe5a1d128f901217",
      "4728b710d66b4dbd8ab4745d305922b2",
      "660d1ab003be4535b337d9663c471443",
      "a79dc2844a864534ba0ef65a3e969ce6",
      "b799c6a11d194976889771f26ef0510f",
      "f063375417be4c4fa0cb79ced7c80494",
      "f299b43c0f624fc49d4f1d06a819a5ba",
      "2e016e9769c74ed99ce210f3383db706",
      "722aa13c6ed142ca9d2b41ef0e7357ae",
      "1a6ed1d76eff492b87e6a01bce3cc94a",
      "c744c48ce3bf493f87361adacad85e48",
      "e846ba9a863e404a95c8be001712458e",
      "b2fe370357ad4b8baed9b22b590b7906",
      "68eb73a1dde04e5e84484fefa3e3eb78",
      "8dbbedb4ac0c4f8f96870d5a45320276",
      "8d796232e4bb4c079eac6bf503af9359",
      "89b1476a064144eda7094bdea348a3a8",
      "21f2825a36494e4fa2c81a74f27377d7",
      "66172511d6b746bda93900e5ffb66177",
      "e35cf53593434cf48aaca2303c5f0bd5",
      "060aa413a2bf49029c7ee3018952f10a",
      "7d883ef3c9964a3a945f54d6d37ac916",
      "46ce1e0fe4e544b7bb0ad62e4432df8a",
      "5eca976384544e17a8cc96c90eda676d",
      "97627b51270940529c41669d3f2eb7c0",
      "8b786faa263c4806aa6fb0116d0d62af",
      "71bcd94e111844e2a795e71ebb56e20e",
      "fbf0f20a216746ac951b21e2ce6a67d0",
      "6824f008b0e745c2ae780fe79f8a540b",
      "d341ba27e42c47d38fc148e84e2b1181",
      "ff351a5e219748c78389d9d574a2d5d1",
      "60541b430c5049fa938319426e42b860",
      "142764b7e35b488a893edf643f6d5ee7",
      "585f6e1f44be4fcf91ac64b6f43ba7eb",
      "0d9d13a8eb054a26b863acaddfe1ce8e",
      "5786e25326f5465db06b955c7b0fc99e",
      "aa23075298f740a2a25bdcedad83b8cf",
      "35fcd41e06ee427b8f39984371054663",
      "2b8222f66fc74695acd197dc0bc8dc11",
      "9c82e13b8f35428594be26d185b4528c",
      "53fa43225ab74a7c868a4e967c174b4c",
      "409eb24c76ef4c62909802a5c3aeff6e",
      "f2d706c9d6954dc5bc35895fefc186a9",
      "e2deb2e8df0640f69736f20b43d20e35",
      "a801f0fac564470ea552df6323d28693",
      "58e2365c4fea4a7ea71758bb7a35cf92",
      "ee8351dd63714604a90c1d4b99377440",
      "7e879c7b068349a59d173105144425cb",
      "7bb56d85c6d04ae5bd49f9246d49155f",
      "b13d988ed54f46d7b5563c8e181e2663",
      "a333a8f51b7145b2b4b0bc80aa24a142",
      "49803f2007524baaa198e890ab7b4138",
      "82a5adc23d4240f08fa1a7bea2186b8a",
      "56b85fcc24c6486da5fd3f5861ac4ead",
      "134a1561b8bd4da68efeb40beaac27b5",
      "2c37355ac86348ea8eea53ebd1d92ba2",
      "efd2b78057084f1a9d8d0b1c5c661dcc",
      "7ae251a8685a45b4b78c50eb0b8af07e",
      "b21f771ee4d24531a1f250a6fcabe4f4",
      "3f18fe732b674686a077977e8a981390",
      "b3b08993d94a40ee9941e82ac4650f49",
      "ca599f2bb3904ebf8e9ce4d1f16702cf",
      "0e1a8c6797c84fa5894cb5edef82092b",
      "d21cd4e1d14e4ab0932a2295fc39a241",
      "0fb62acc3323490e998936ad23fddb42",
      "d3790e958dc64af0b03175fab25b413f",
      "353094b7bb44439486c53677fe9c2d79"
     ]
    },
    "execution": {
     "iopub.execute_input": "2024-08-02T14:42:44.247987Z",
     "iopub.status.busy": "2024-08-02T14:42:44.247635Z",
     "iopub.status.idle": "2024-08-02T14:43:23.248010Z",
     "shell.execute_reply": "2024-08-02T14:43:23.247187Z",
     "shell.execute_reply.started": "2024-08-02T14:42:44.247952Z"
    },
    "id": "pLF1nVR3nfCz",
    "outputId": "e414cfdd-bcfb-44ff-c7d2-c8c04a465266"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64486fcfdaab4b9eafcb22a9043a86ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/15.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49eaf804cf4744a2a4427feca336e8e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/257M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f15f2733215b444a8c5107da5d06a4ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/257M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4728b710d66b4dbd8ab4745d305922b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/259M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2fe370357ad4b8baed9b22b590b7906",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/34.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eca976384544e17a8cc96c90eda676d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/30.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d9d13a8eb054a26b863acaddfe1ce8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/287113 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58e2365c4fea4a7ea71758bb7a35cf92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/13368 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efd2b78057084f1a9d8d0b1c5c661dcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/11490 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data = datasets.load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2hjaBi3ZnfCz"
   },
   "source": [
    "### Custom recipes for DataFrame inspection\n",
    "Object predictions will be stored in a dataframe so it can be displayed in a friendly manner in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-02T14:43:23.249719Z",
     "iopub.status.busy": "2024-08-02T14:43:23.249384Z",
     "iopub.status.idle": "2024-08-02T14:43:23.255187Z",
     "shell.execute_reply": "2024-08-02T14:43:23.253997Z",
     "shell.execute_reply.started": "2024-08-02T14:43:23.249690Z"
    },
    "id": "Uc7h0LP2nfCz"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.precision', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__voSmFgnfC0"
   },
   "source": [
    "### Understanding the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ZWd4fGgnfC0"
   },
   "source": [
    "##### Inspecting dataset's metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-08-02T14:43:23.256791Z",
     "iopub.status.busy": "2024-08-02T14:43:23.256471Z",
     "iopub.status.idle": "2024-08-02T14:43:23.270215Z",
     "shell.execute_reply": "2024-08-02T14:43:23.269135Z",
     "shell.execute_reply.started": "2024-08-02T14:43:23.256756Z"
    },
    "id": "gxoAkBjknfC0",
    "outputId": "2a2853e2-b794-4985-bf8b-6340711e261d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['article', 'highlights', 'id'],\n",
       "    num_rows: 287113\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-08-02T14:43:23.272111Z",
     "iopub.status.busy": "2024-08-02T14:43:23.271528Z",
     "iopub.status.idle": "2024-08-02T14:43:23.281402Z",
     "shell.execute_reply": "2024-08-02T14:43:23.280496Z",
     "shell.execute_reply.started": "2024-08-02T14:43:23.272077Z"
    },
    "id": "xczTVL-9nfC0",
    "outputId": "a576971d-dddb-4a61-de4a-aa599e2548ca",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'article': Value(dtype='string', id=None),\n",
       " 'highlights': Value(dtype='string', id=None),\n",
       " 'id': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-08-02T14:43:23.282845Z",
     "iopub.status.busy": "2024-08-02T14:43:23.282485Z",
     "iopub.status.idle": "2024-08-02T14:43:23.294434Z",
     "shell.execute_reply": "2024-08-02T14:43:23.293416Z",
     "shell.execute_reply.started": "2024-08-02T14:43:23.282818Z"
    },
    "id": "8jpjSbSpnfC1",
    "outputId": "75e9ad28-7359-46f7-b264-c26443808eda"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': SplitInfo(name='train', num_bytes=1261703785, num_examples=287113, shard_lengths=[115705, 115704, 55704], dataset_name='cnn_dailymail'),\n",
       " 'validation': SplitInfo(name='validation', num_bytes=57732412, num_examples=13368, shard_lengths=None, dataset_name='cnn_dailymail'),\n",
       " 'test': SplitInfo(name='test', num_bytes=49925732, num_examples=11490, shard_lengths=None, dataset_name='cnn_dailymail')}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.info.splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ppzh9ljfnfC1"
   },
   "source": [
    "So far we see our dataset is already split into train, validation and test with 287113, 13368, 11490 examples respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KolwE429nfC1"
   },
   "source": [
    "##### Peek at training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N9MNYmGNnfC2"
   },
   "source": [
    "Our input is called article and our labels are called highlights. Let's now print out the first example of the training data to get a feeling for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 566
    },
    "execution": {
     "iopub.execute_input": "2024-08-02T14:43:23.298579Z",
     "iopub.status.busy": "2024-08-02T14:43:23.298240Z",
     "iopub.status.idle": "2024-08-02T14:43:23.322183Z",
     "shell.execute_reply": "2024-08-02T14:43:23.320985Z",
     "shell.execute_reply.started": "2024-08-02T14:43:23.298543Z"
    },
    "id": "OoNltCagnfC2",
    "outputId": "7e571236-96c5-492f-a732-4d94db9ff835"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>highlights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won't cast a spell on him. Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \"I don't plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\" he told an Australian interviewer earlier this month. \"I don't think I'll be particularly extravagant. \"The things I like buying are things that cost about 10 pounds -- books and CDs and DVDs.\" At 18, Radcliffe will be able to gamble in a casino, buy a drink in a pub or see the horror film \"Hostel: Part II,\" currently six places below his number one movie on the UK box office chart. Details of how he'll mark his landmark birthday are under wraps. His agent and publicist had no comment on his plans. \"I'll definitely have some sort of party,\" he said in an interview. \"Hopefully none of you will be reading about it.\" Radcliffe's earnings from the first five Potter films have been held in a trust fund which he has not been able to touch. Despite his growing fame and riches, the actor says he is keeping his feet firmly on the ground. \"People are always looking to say 'kid star goes off the rails,'\" he told reporters last month. \"But I try very hard not to go that way because it would be too easy for them.\" His latest outing as the boy wizard in \"Harry Potter and the Order of the Phoenix\" is breaking records on both sides of the Atlantic and he will reprise the role in the last two films.  Watch I-Reporter give her review of Potter's latest » . There is life beyond Potter, however. The Londoner has filmed a TV movie called \"My Boy Jack,\" about author Rudyard Kipling and his son, due for release later this year. He will also appear in \"December Boys,\" an Australian film about four boys who escape an orphanage. Earlier this year, he made his stage debut playing a tortured teenager in Peter Shaffer's \"Equus.\" Meanwhile, he is braced for even closer media scrutiny now that he's legally an adult: \"I just think I'm going to be more sort of fair game,\" he told Reuters. E-mail to a friend . Copyright 2007 Reuters. All rights reserved.This material may not be published, broadcast, rewritten, or redistributed.</td>\n",
       "      <td>Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday .\\nYoung actor says he has no plans to fritter his cash away .\\nRadcliffe's earnings from first five Potter films have been held in trust fund .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(train_data[:1])\n",
    "del df[\"id\"]\n",
    "for column, typ in train_data.features.items():\n",
    "      if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F3HMPi9mnfC2"
   },
   "source": [
    "The input data seems to consist of short news articles. Interestingly, the labels appear to be bullet-point-like summaries. At this point, we should probably take a look at a couple of other examples to get a better feeling for the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "execution": {
     "iopub.execute_input": "2024-08-02T14:43:23.324269Z",
     "iopub.status.busy": "2024-08-02T14:43:23.323935Z",
     "iopub.status.idle": "2024-08-02T14:43:23.334253Z",
     "shell.execute_reply": "2024-08-02T14:43:23.333293Z",
     "shell.execute_reply.started": "2024-08-02T14:43:23.324242Z"
    },
    "id": "G93_pUjZnfC2",
    "outputId": "5f21fac3-44bb-4f9f-af41-be29215a9058"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>highlights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Editor's note: In our Behind the Scenes series, CNN correspondents share their experiences in covering news and analyze the stories behind the events. Here, Soledad O'Brien takes users inside a jail where many of the inmates are mentally ill. An inmate housed on the \"forgotten floor,\" where many mentally ill inmates are housed in Miami before trial. MIAMI, Florida (CNN) -- The ninth floor of the Miami-Dade pretrial detention facility is dubbed the \"forgotten floor.\" Here, inmates with the most severe mental illnesses are incarcerated until they're ready to appear in court. Most often, they face drug charges or charges of assaulting an officer --charges that Judge Steven Leifman says are usually \"avoidable felonies.\" He says the arrests often result from confrontations with police. Mentally ill people often won't do what they're told when police arrive on the scene -- confrontation seems to exacerbate their illness and they become more paranoid, delusional, and less likely to follow directions, according to Leifman. So, they end up on the ninth floor severely mentally disturbed, but not getting any real help because they're in jail. We toured the jail with Leifman. He is well known in Miami as an advocate for justice and the mentally ill. Even though we were not exactly welcomed with open arms by the guards, we were given permission to shoot videotape and tour the floor.  Go inside the 'forgotten floor' » . At first, it's hard to determine where the people are. The prisoners are wearing sleeveless robes. Imagine cutting holes for arms and feet in a heavy wool sleeping bag -- that's kind of what they look like. They're designed to keep the mentally ill patients from injuring themselves. That's also why they have no shoes, laces or mattresses. Leifman says about one-third of all people in Miami-Dade county jails are mentally ill. So, he says, the sheer volume is overwhelming the system, and the result is what we see on the ninth floor. Of course, it is a jail, so it's not supposed to be warm and comforting, but the lights glare, the cells are tiny and it's loud. We see two, sometimes three men -- sometimes in the robes, sometimes naked, lying or sitting in their cells. \"I am the son of the president. You need to get me out of here!\" one man shouts at me. He is absolutely serious, convinced that help is on the way -- if only he could reach the White House. Leifman tells me that these prisoner-patients will often circulate through the system, occasionally stabilizing in a mental hospital, only to return to jail to face their charges. It's brutally unjust, in his mind, and he has become a strong advocate for changing things in Miami. Over a meal later, we talk about how things got this way for mental patients. Leifman says 200 years ago people were considered \"lunatics\" and they were locked up in jails even if they had no charges against them. They were just considered unfit to be in society. Over the years, he says, there was some public outcry, and the mentally ill were moved out of jails and into hospitals. But Leifman says many of these mental hospitals were so horrible they were shut down. Where did the patients go? Nowhere. The streets. They became, in many cases, the homeless, he says. They never got treatment. Leifman says in 1955 there were more than half a million people in state mental hospitals, and today that number has been reduced 90 percent, and 40,000 to 50,000 people are in mental hospitals. The judge says he's working to change this. Starting in 2008, many inmates who would otherwise have been brought to the \"forgotten floor\"  will instead be sent to a new mental health facility -- the first step on a journey toward long-term treatment, not just punishment. Leifman says it's not the complete answer, but it's a start. Leifman says the best part is that it's a win-win solution. The patients win, the families are relieved, and the state saves money by simply not cycling these prisoners through again and again. And, for Leifman, justice is served. E-mail to a friend .</td>\n",
       "      <td>Mentally ill inmates in Miami are housed on the \"forgotten floor\"\\nJudge Steven Leifman says most are there as a result of \"avoidable felonies\"\\nWhile CNN tours facility, patient shouts: \"I am the son of the president\"\\nLeifman says the system is unjust and he's fighting for change .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MINNEAPOLIS, Minnesota (CNN) -- Drivers who were on the Minneapolis bridge when it collapsed told harrowing tales of survival. \"The whole bridge from one side of the Mississippi to the other just completely gave way, fell all the way down,\" survivor Gary Babineau told CNN. \"I probably had a 30-, 35-foot free fall. And there's cars in the water, there's cars on fire. The whole bridge is down.\" He said his back was injured but he determined he could move around. \"I realized there was a school bus right next to me, and me and a couple of other guys went over and started lifting the kids off the bridge. They were yelling, screaming, bleeding. I think there were some broken bones.\"  Watch a driver describe his narrow escape » . At home when he heard about the disaster, Dr. John Hink, an emergency room physician, jumped into his car and rushed to the scene in 15 minutes. He arrived at the south side of the bridge, stood on the riverbank and saw dozens of people lying dazed on an expansive deck. They were in the middle of the Mississippi River, which was churning fast, and he had no way of getting to them. He went to the north side, where there was easier access to people. Ambulances were also having a hard time driving down to the river to get closer to the scene. Working feverishly, volunteers, EMTs and other officials managed to get 55 people into ambulances in less than two hours. Occasionally, a pickup truck with a medic inside would drive to get an injured person and bring him back up even ground, Hink told CNN. The rescue effort was controlled and organized, he said; the opposite of the lightning-quick collapse. \"I could see the whole bridge as it was going down, as it was falling,\" Babineau said. \"It just gave a rumble real quick, and it all just gave way, and it just fell completely, all the way to the ground. And there was dust everywhere and it was just like everyone has been saying: It was just like out of the movies.\" Babineau said the rear of his pickup truck was dangling over the edge of a broken-off section of the bridge. He said several vehicles slid past him into the water. \"I stayed in my car for one or two seconds. I saw a couple cars fall,\" he said. \"So I stayed in my car until the cars quit falling for a second, then I got out real quick, ran in front of my truck -- because behind my truck was just a hole -- and I helped a woman off of the bridge with me. \"I just wanted off the bridge, and then I ran over to the school bus. I started grabbing kids and handing them down. It was just complete chaos.\" He said most of the children were crying or screaming. He and other rescuers set them on the ground and told them to run to the river bank, but a few needed to be carried because of their injuries.  See rescuers clamber over rubble » . Babineau said he had no rescue training. \"I just knew what I had to do at the moment.\" Melissa Hughes, 32, of Minneapolis, told The Associated Press that she was driving home when the western edge of the bridge collapsed under her. \"You know that free-fall feeling? I felt that twice,\" Hughes said. A pickup landed on top of her car, but she was not hurt. \"I had no idea there was a vehicle on my car,\" she told AP. \"It's really very surreal.\" Babineau told the Minneapolis Star-Tribune: \"On the way down, I thought I was dead. I literally thought I was dead. \"My truck was completely face down, pointed toward the ground, and my truck got ripped in half. It was folded in half, and I can't believe I'm alive.\"  See and hear eyewitness accounts » . Bernie Toivonen told CNN's \"American Morning\" that his vehicle was on a part of the bridge that ended up tilted at a 45-degree angle. \"I knew the deck was going down, there was no question about it, and I thought I was going to die,\" he said. After the bridge settled and his car remained upright, \"I just put in park, turned the key off and said, 'Oh, I'm alive,' \" he said. E-mail to a friend .</td>\n",
       "      <td>NEW: \"I thought I was going to die,\" driver says .\\nMan says pickup truck was folded in half; he just has cut on face .\\nDriver: \"I probably had a 30-, 35-foot free fall\"\\nMinnesota bridge collapsed during rush hour Wednesday .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WASHINGTON (CNN) -- Doctors removed five small polyps from President Bush's colon on Saturday, and \"none appeared worrisome,\" a White House spokesman said. The polyps were removed and sent to the National Naval Medical Center in Bethesda, Maryland, for routine microscopic examination, spokesman Scott Stanzel said. Results are expected in two to three days. All were small, less than a centimeter [half an inch] in diameter, he said. Bush is in good humor, Stanzel said, and will resume his activities at Camp David. During the procedure Vice President Dick Cheney assumed presidential power. Bush reclaimed presidential power at 9:21 a.m. after about two hours. Doctors used \"monitored anesthesia care,\" Stanzel said, so the president was asleep, but not as deeply unconscious as with a true general anesthetic. He spoke to first lady Laura Bush -- who is in Midland, Texas, celebrating her mother's birthday -- before and after the procedure, Stanzel said. Afterward, the president played with his Scottish terriers, Barney and Miss Beazley, Stanzel said. He planned to have lunch at Camp David and have briefings with National Security Adviser Stephen Hadley and White House Chief of Staff Josh Bolten, and planned to take a bicycle ride Saturday afternoon. Cheney, meanwhile, spent the morning at his home on Maryland's eastern shore, reading and playing with his dogs, Stanzel said. Nothing occurred that required him to take official action as president before Bush reclaimed presidential power. The procedure was supervised by Dr. Richard Tubb, Bush's physician, and conducted by a multidisciplinary team from the National Naval Medical Center in Bethesda, Maryland, the White House said. Bush's last colonoscopy was in June 2002, and no abnormalities were found, White House spokesman Tony Snow said. The president's doctor had recommended a repeat procedure in about five years. A colonoscopy is the most sensitive test for colon cancer, rectal cancer and polyps, small clumps of cells that can become cancerous, according to the Mayo Clinic. Small polyps may be removed during the procedure. Snow said on Friday that Bush had polyps removed during colonoscopies before becoming president. Snow himself is undergoing chemotherapy for cancer that began in his colon and spread to his liver.  Watch Snow talk about Bush's procedure and his own colon cancer » . \"The president wants to encourage everybody to use surveillance,\" Snow said. The American Cancer Society recommends that people without high risk factors or symptoms begin getting screened for signs of colorectal cancer at age 50. E-mail to a friend .</td>\n",
       "      <td>Five small polyps found during procedure; \"none worrisome,\" spokesman says .\\nPresident reclaims powers transferred to vice president .\\nBush undergoes routine colonoscopy at Camp David .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(train_data[1:4])\n",
    "del df[\"id\"]\n",
    "for column, typ in train_data.features.items():\n",
    "      if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MtE0RpFVnfC3"
   },
   "source": [
    "Next, let's get a sense of the length of input data and labels.  \n",
    "As models compute length in *token-length*, we will make use of the `bert-base-cased` tokenizer to compute the article and summary length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nzXIcqoRnfC3"
   },
   "source": [
    "##### Quick statistics\n",
    "Artilce and Summary Length,\n",
    "Histogram Plots,\n",
    "Correlation Matrix,\n",
    "Categorical Variable Selection &\n",
    "Important Numeric Features from PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "f23e39a6604c42c7aacf041b4fd202e3",
      "45ad89ca7a53486e9d08db115191dcdc",
      "3a3d535b6b7c4061a386eb7b37ac89d8",
      "9047bd9a39304138a3f3e145d52400d1",
      "f92ae8c43e5b408ebb032cf9f02e48a0",
      "e20d7573152c4f02a5c3001f241d4fa2",
      "5adc07c1cda3444cac4d93ab684ebbfc",
      "c385ee4758b94f4895fdbb624fddb9cc",
      "7566afcb6c5849cbaeaf87410c48c4f0",
      "d0d75082a9e945109406560d93b1bfd4",
      "71d7fcb9a13b476ea75da9e92d6fff1a",
      "5d368944bc714547a7bbf7edbebeed6f",
      "ba945ca79af64df7824eca55f8d7339b",
      "ccbb273d259046cf8bc180d9e94fd37b",
      "66d0256e4630432b9ec6c7c38835c25c",
      "da9d2399c6fd4a99bdda284284560986",
      "bb57c62915ca4547a59ce983bc5b8014",
      "6a456c5125ee4f1aad32252fe96e0725",
      "5da993d90d1b4520b28da29517e49105",
      "8dd61ccfcd69448eafdaa938d8564320",
      "9e1da832a0c741938ea21b096a5a995b",
      "0569e58168e94578bb8d27d02dbe7530",
      "c4ad1bd5943b44b48daffd08d66f3d7d",
      "8fff7e3e1b294336861643cba3e3b365",
      "107d5eeb730c4885840f8aaed480e0ba",
      "8b02b5005ad647d6b3ddd91512bc5eca",
      "b735dd5a57c3462ba94258ffe034f642",
      "2b2ece127d1b483088b295cc4f88def0",
      "2ea70995fa434c958e4a30eeb3dbca54",
      "67d32e2d708b4686a1a10f81a2774548",
      "ada0f93a20b04ab1a1a09c3241d60347",
      "c7e93e48476044039d2d3160f8028a1a",
      "3b6c09a3820c4dc9aafca0a6a72d4e57",
      "df1bd22f0785453191d85b3f43332c5a",
      "2a3abce22fc842349c036f5a9dff9873",
      "7e443ff89cec461ca80c057aea3d62c3",
      "b6f40a51494e4e05b467c3954d8b023a",
      "26457d241b3c4fd69f6b3900f67eb66f",
      "906bb751514f4226bd7d93bc4159406d",
      "ec24cf38c5914569a415af22839d0fa0",
      "32a5d96b2f0848ebb7f35434050245fc",
      "5d32786759e4452bb14afa62bad3ce21",
      "288feeef610d46db8239331b033fe28b",
      "74388403202e44f48aa2ba29a53ce9c3"
     ]
    },
    "execution": {
     "iopub.execute_input": "2024-08-02T14:43:23.336015Z",
     "iopub.status.busy": "2024-08-02T14:43:23.335678Z",
     "iopub.status.idle": "2024-08-02T14:43:24.054529Z",
     "shell.execute_reply": "2024-08-02T14:43:24.053645Z",
     "shell.execute_reply.started": "2024-08-02T14:43:23.335988Z"
    },
    "id": "M0FWEkj8LThp",
    "outputId": "3c69a772-d2ff-456c-b7b1-c9311e5192be",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f23e39a6604c42c7aacf041b4fd202e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d368944bc714547a7bbf7edbebeed6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4ad1bd5943b44b48daffd08d66f3d7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df1bd22f0785453191d85b3f43332c5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uUNOm0AALxLA"
   },
   "source": [
    "Next, we make use of `.map()` to compute the length of the article and its summary. The maximum length that `bert-base-uncased` can process amounts to 512, we are also interested in the percentage of input samples being longer than the maximum length.\n",
    "Similarly, we compute the percentage of summaries that are longer than 64, and 128 respectively as these will help us meet our goal of providing multi-paragraph summaries if possible\n",
    "\n",
    "We can define the `.map()` function as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-02T14:43:24.056186Z",
     "iopub.status.busy": "2024-08-02T14:43:24.055827Z",
     "iopub.status.idle": "2024-08-02T14:43:24.062730Z",
     "shell.execute_reply": "2024-08-02T14:43:24.061598Z",
     "shell.execute_reply.started": "2024-08-02T14:43:24.056156Z"
    },
    "id": "wakECgirV1ZN"
   },
   "outputs": [],
   "source": [
    "# map article and summary len to dict as well as if sample is longer than 512 tokens\n",
    "def map_to_length(x):\n",
    "    x[\"article_len\"] = len(tokenizer(x[\"article\"]).input_ids)\n",
    "    x[\"article_longer_512\"] = int(x[\"article_len\"] > tokenizer.model_max_length)\n",
    "    x[\"summary_len\"] = len(tokenizer(x[\"highlights\"]).input_ids)\n",
    "    x[\"summary_longer_64\"] = int(x[\"summary_len\"] > 64)\n",
    "    x[\"summary_longer_128\"] = int(x[\"summary_len\"] > 128)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GkzX1bM1N6oz"
   },
   "source": [
    "It should be sufficient to look at the first 10000 samples. We can speed up the mapping by using multiple processes with `num_proc=4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104,
     "referenced_widgets": [
      "a802355e8bf944018ca7bed358c12efd",
      "a4a40c974e8e402aac0e5db4b4cd465d",
      "bc7b0eb4e39a461690b2770f1bf92bc1",
      "97cfff7da6ef4f5cb4471f1e98ca28de",
      "7d89238925e945ecb8a0c09ea4dec655",
      "f9b696c815054c248142ac10834d5b4c",
      "969ffda21b0744b8bb1ef32eea3d518c",
      "f0a7d0f03c084588a9ee2584faed156b",
      "058c5e1b6f5b4616b67f7699771e4a3f",
      "1af439a718a14f8386261cba88f2fb92",
      "8db8ad59a116494db6fb87e5acca26db"
     ]
    },
    "execution": {
     "iopub.execute_input": "2024-08-02T14:43:24.064633Z",
     "iopub.status.busy": "2024-08-02T14:43:24.064282Z",
     "iopub.status.idle": "2024-08-02T14:43:38.379148Z",
     "shell.execute_reply": "2024-08-02T14:43:38.378043Z",
     "shell.execute_reply.started": "2024-08-02T14:43:24.064605Z"
    },
    "id": "5yAVH5fkMwLr",
    "outputId": "560342a4-faac-4d3c-b38b-42d40c0e16f0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a802355e8bf944018ca7bed358c12efd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_size = 10000\n",
    "data_stats = train_data.select(range(sample_size)).map(map_to_length, num_proc=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3eluYonnRL-c"
   },
   "source": [
    "Having computed the length for the first 10000 samples, we can now average them together. We can make use of the `.map()` function with `batched=True` and `batch_size=-1` to have access to all 10000 samples within the `.map()` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xM-S81_YnfC6"
   },
   "source": [
    "We have also identified that some articles are too long and can cause training errors we will `use.filter()` to remove them in a data cleaning section below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86,
     "referenced_widgets": [
      "e25bdd34c85d44508982fee69c68b36f",
      "d40fff9080d0430583369bfe230581f8",
      "ab902e0be28f4bedad9e21bae83e9ca1",
      "c42ed10aa4b44b239b5a1986a0d16c96",
      "0f280dd1d7bf423fa46aa5671cf63177",
      "3be81746c5ba41a1a6003ff1efcd0bc8",
      "c1a02db2d55044be82e44425ae48fe4d",
      "dcf3991f81d34e5895b18a54bc281aef",
      "c004dcb156fe4d63b74317d358175442",
      "f589e4dcbd5b43de9b22a572947f847e",
      "bbc27041a03748e483f91fe2bcfe7401"
     ]
    },
    "execution": {
     "iopub.execute_input": "2024-08-02T14:43:38.381570Z",
     "iopub.status.busy": "2024-08-02T14:43:38.381144Z",
     "iopub.status.idle": "2024-08-02T14:43:38.450622Z",
     "shell.execute_reply": "2024-08-02T14:43:38.449623Z",
     "shell.execute_reply.started": "2024-08-02T14:43:38.381523Z"
    },
    "id": "kALWVxHlOU4A",
    "outputId": "148a911a-db68-42dd-8203-693f7772081f",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e25bdd34c85d44508982fee69c68b36f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article Mean: 827.079, %-Articles > 512:0.7344, Summary Mean:59.5725, %-Summary > 64:0.3499, %-Summary > 128:0.0\n"
     ]
    }
   ],
   "source": [
    "def compute_and_print_stats(x):\n",
    "    if len(x[\"article_len\"]) == sample_size:\n",
    "        print(\n",
    "            \"Article Mean: {}, %-Articles > 512:{}, Summary Mean:{}, %-Summary > 64:{}, %-Summary > 128:{}\".format(\n",
    "                sum(x[\"article_len\"]) / sample_size,\n",
    "                sum(x[\"article_longer_512\"]) / sample_size,\n",
    "                sum(x[\"summary_len\"]) / sample_size,\n",
    "                sum(x[\"summary_longer_64\"]) / sample_size,\n",
    "                sum(x[\"summary_longer_128\"]) / sample_size,\n",
    "            )\n",
    "        )\n",
    "\n",
    "output = data_stats.map(compute_and_print_stats, batched=True, batch_size=-1,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQYzSmTYnfC6"
   },
   "source": [
    "We can see that on average an article contains 827 tokens with *ca.* 3/4 of the articles being longer than the model's `max_length` 512. The summary is on average 59 tokens long. About 35% of our 10000-sample summaries are longer than 64 tokens, but none are longer than 128 tokens.\n",
    "\n",
    "`bert-base-cased` is limited to 512 tokens, which means we would have to cut possibly important information from the article. Because most of the important information is often found at the beginning of articles and because we want to be computationally efficient, we decide to stick to `bert-base-cased` with a `max_length` of 512 in this notebook. This choice is not optimal but has shown to yield [good results](https://arxiv.org/abs/1907.12461) on CNN/Dailymail.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6yGVlXLmnfC8"
   },
   "source": [
    "#### Some notes so far\n",
    "\n",
    "1. Evaluation :   \n",
    "\n",
    "    **Case-sensitivity**\n",
    "    - The text in our peeks were *case-sensitive*. This means that we have to be careful if we want to use *case-insensitive* models. As *CNN/Dailymail* is a summarization dataset, the model will be evaluated using the *ROUGE* metric. Checking the description of *ROUGE* in 🤗datasets, *cf.* [here](https://huggingface.co/metrics/rouge), we can see that the metric is *case-insensitive*, meaning that *upper case* letters will be normalized to *lower case* letters during evaluation. Thus, we can safely leverage *uncased* checkpoints, such as `bert-base-uncased`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d82755f5-c6f8-4d0b-8cef-accc84a91e84"
   },
   "source": [
    "# Task 1\n",
    "Test the pretrained BERT-Based model for summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "chhni9zWnfC9"
   },
   "source": [
    "Why use bert2BERT and not BERT_BASE?  \n",
    "bert2BERT is a Reusable Pretrained Language Models\n",
    "In recent years, researchers tend to pre-train ever-larger language models to explore the upper limit of deep models. However, large language model pre-training costs intensive computational resources and most of the models are trained from scratch without reusing the existing pre-trained models, which is wasteful. bert2BERT allows us to  effectively transfer the knowledge of an existing smaller pre-trained model like BERT_BASE to a large model  through parameter initialization and significantly improve the pre-training efficiency of the large model. [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "glnMLtXOnfC9"
   },
   "source": [
    "**Loading a current default summarization model bart-large from the summarization pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209,
     "referenced_widgets": [
      "f8d64c9d65af4bfaa9fc96f0f3a029ea",
      "6c2e5d0e436b4e81ae3b380b353c3b6d",
      "d43bfdd3723e4e7ab47c19a9f456c804",
      "4b5fd52a7e8f481a8a34287ed68351c5",
      "6f341bfdd6d4422a935f267cd828e446",
      "b5c1420ad9f14933985684034ffa0c7f",
      "7a248e781810425b87a7e677f9a912dd",
      "014605fb79c649388441821f13a14c09",
      "21364353cb29455d93e31c28f0ccbf5b",
      "38d6da7f01304cfd8a970ee9be8ec66c",
      "b5739fd211c4417186f2340f2bb87d47",
      "6bab3f4a21434b8db5dc94f41eef7408",
      "9127bee9ff154b4e90cc13df5c03917d",
      "a4f188ab47a041caa7837a64f0b53df2",
      "28e6452e038c41b98af91defb7ba94ba",
      "18999d7ca0a241359be282616040c10e",
      "f3e7d1975e49462784511febbab6cbd4",
      "24984e0c46be476592f01d588c809c3c",
      "3fbb383fa2ef46fa94bf5e7353370992",
      "92300495507b430abd18a6fe05f83e11",
      "94619bba26154414bf5ab2f482053dd1",
      "cacc0746f5b74ee98f3b8ce4543bd647",
      "30de766c5ed241a29784552297580b22",
      "9f04321eb3a34200bc2e8e535bec83fb",
      "b8e236407ad940649cef5ddeaf4f8203",
      "caa69ce8a3204521bb0329cfde9e44a6",
      "a75f20569e5e4f768157eb3e3687d281",
      "492e4873cbcb49f2aca9799138b21a0c",
      "a12dd3eac87141b6822303e8dfa4177d",
      "0180d088a295497eb4ad4fd6fac06bda",
      "da7548e69067422bb0c043ab3fe8e2fe",
      "443d0308c47842359b67af387a480462",
      "63dc9551450a46e59add0dbeb1ae918e",
      "55e19bef523b4cf49860946bcd5bd62a",
      "bd6bac7ceb3e4fa3a7fb1fb0ca669eeb",
      "58afee0ad15d4895ab8dfe53c6d41059",
      "4cfcdd9887a24f53827a0a4b49bf0993",
      "aeb1daa7ede94d6e927acb3358d66824",
      "7d282d33c9934dfe8188b68b36fe5b6f",
      "d83f5920425741e09c4045f1ff3a1a0d",
      "f92db94643424aff9e4d221422bade34",
      "87ffd30e31f843bfb9d97a7d6cabf137",
      "ba4a30a76738490fb91a81523b1d9032",
      "901fdf6b535c4423951e0828fe753711",
      "b62650fe7e7f438e827414080caaaa15",
      "3fce33d48171426eb72ff0dd2d734073",
      "a4e20729fe72400aaedc0f0f0e5468a9",
      "477da511d50e4bc3987bb7b726b31590",
      "cab5520ddd614341ae1b4f1b8e624b32",
      "fd3d0d8c0bf741ef83113b39233e20df",
      "edc6c4f722f748098597d536c9b843ac",
      "c1a18bb728854cb597e8d8e13ae45a52",
      "cc999ab497d0465f94f781c127ea070c",
      "4dca455b42ef44e08089be8474fe965a",
      "c50929203df5450088b9054a50b51f34",
      "f1f5151e0a064bda91e93b786acc4917",
      "e14b5dafb9244482a059c3cffca46b7f",
      "812887f156e74785a7d8e4171b85d329",
      "ae6483151774429ab5b332396288e6b9",
      "652ccfe08801468c85340edb5d8d8452",
      "f8938fea2b6c434ca3d7ba04fa2e9929",
      "a1f4b044f87046ba970184b6bfcd9b5d",
      "d8a102d7609d4ba59f54ca3e85d9231a",
      "b772d4cfa5734a2eb6ef3a6ede187312",
      "4ebe1775728f4b6c943b4663b68931a6",
      "e958e01ae0184c6db5386362036d4584"
     ]
    },
    "execution": {
     "iopub.execute_input": "2024-08-02T14:43:38.472564Z",
     "iopub.status.busy": "2024-08-02T14:43:38.472177Z",
     "iopub.status.idle": "2024-08-02T14:44:20.280002Z",
     "shell.execute_reply": "2024-08-02T14:44:20.279086Z",
     "shell.execute_reply.started": "2024-08-02T14:43:38.472519Z"
    },
    "id": "Xn1Uu3qqnfC-",
    "outputId": "2d9a8458-c092-4e55-cbbb-aca09fd2384a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d64c9d65af4bfaa9fc96f0f3a029ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bab3f4a21434b8db5dc94f41eef7408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30de766c5ed241a29784552297580b22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55e19bef523b4cf49860946bcd5bd62a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b62650fe7e7f438e827414080caaaa15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1f5151e0a064bda91e93b786acc4917",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\",device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-02T14:44:20.282244Z",
     "iopub.status.busy": "2024-08-02T14:44:20.281513Z",
     "iopub.status.idle": "2024-08-02T14:44:20.289697Z",
     "shell.execute_reply": "2024-08-02T14:44:20.288493Z",
     "shell.execute_reply.started": "2024-08-02T14:44:20.282205Z"
    },
    "id": "DmuK-olYnfC-"
   },
   "outputs": [],
   "source": [
    "def summarize_article(article, tokenizer, summarizer):\n",
    "    inputs = tokenizer(article, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    inputs = {key: val.to(\"cuda\") for key, val in inputs.items()}  # Ensure inputs are on GPU\n",
    "    summary = summarizer(article, max_length=150, min_length=50, do_sample=True)\n",
    "    return summary[0]['summary_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2t_tCh39nfC-"
   },
   "source": [
    "**Generating a few sample summaries with it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-02T14:44:20.291567Z",
     "iopub.status.busy": "2024-08-02T14:44:20.291155Z",
     "iopub.status.idle": "2024-08-02T14:44:20.309241Z",
     "shell.execute_reply": "2024-08-02T14:44:20.308230Z",
     "shell.execute_reply.started": "2024-08-02T14:44:20.291521Z"
    },
    "id": "3NMcMfNinfC_"
   },
   "outputs": [],
   "source": [
    "news_articles = [example[\"article\"] for example in train_data.select(range(3))]\n",
    "target_summaries = [example[\"highlights\"] for example in train_data.select(range(3))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hoB1GAGEnfC_"
   },
   "source": [
    "Summarize the articles and store summaries in summaries list. We set `do_sample` to `True` to avoid greedy decoding and provide a more natural, human-like summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-02T14:44:20.311106Z",
     "iopub.status.busy": "2024-08-02T14:44:20.310649Z",
     "iopub.status.idle": "2024-08-02T14:44:24.005905Z",
     "shell.execute_reply": "2024-08-02T14:44:24.004770Z",
     "shell.execute_reply.started": "2024-08-02T14:44:20.311067Z"
    },
    "id": "y7RzH2QEnfC_"
   },
   "outputs": [],
   "source": [
    "pre_train_summaries = []\n",
    "for article in news_articles:\n",
    "    try:\n",
    "        summary = summarize_article(article, tokenizer, summarizer)\n",
    "        pre_train_summaries.append(summary)\n",
    "    except Exception as e:\n",
    "        pre_train_summaries.append(\"Error, article was too long\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ei6cX9bYnfDA"
   },
   "source": [
    "Create a dataframe containing the pretrained BART summaries and the target summaries from our fine-tuning dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-02T14:44:24.007754Z",
     "iopub.status.busy": "2024-08-02T14:44:24.007349Z",
     "iopub.status.idle": "2024-08-02T14:44:24.013532Z",
     "shell.execute_reply": "2024-08-02T14:44:24.012458Z",
     "shell.execute_reply.started": "2024-08-02T14:44:24.007715Z"
    },
    "id": "l13hQ2v7nfDA"
   },
   "outputs": [],
   "source": [
    "summaries = pd.DataFrame({\n",
    "    'Pre-trained BART Summaries': pre_train_summaries,\n",
    "    'Target Summaries': target_summaries\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 456
    },
    "execution": {
     "iopub.execute_input": "2024-08-02T14:44:24.015136Z",
     "iopub.status.busy": "2024-08-02T14:44:24.014814Z",
     "iopub.status.idle": "2024-08-02T14:44:24.033905Z",
     "shell.execute_reply": "2024-08-02T14:44:24.032251Z",
     "shell.execute_reply.started": "2024-08-02T14:44:24.015109Z"
    },
    "id": "sHUGvO77nfDA",
    "outputId": "08bbadd6-6b3e-4256-ca6d-16122ea12375"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"summaries\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"Pre-trained BART Summaries\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Harry Potter star Daniel Radcliffe turns 18 on Monday. He gains access to a reported \\u00a320 million ($41.1 million) fortune. Radcliffe's earnings from the first five Potter films have been held in a trust fund. Details of how he'll mark his landmark birthday are under wraps.\",\n          \"Judge Steven Leifman is an advocate for justice and the mentally ill. About one-third of all people in Miami-Dade county jails are mentally ill, he says. He says the sheer volume is overwhelming the system. Starting in 2008, many inmates will be sent to a new mental health facility.\",\n          \"NEW: \\\"I probably had a 30-, 35-foot free fall,\\\" survivor Gary Babineau says. NEW: \\\"My truck was completely face down, pointed toward the ground, and my truck got ripped in half,\\\" he says. Dr. John Hink says he saw dozens of people lying dazed on an expansive deck. \\\"It just gave a rumble real quick, and it all just gave way,\\\" survivor says.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Target Summaries\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Harry Potter star Daniel Radcliffe gets \\u00a320M fortune as he turns 18 Monday .\\nYoung actor says he has no plans to fritter his cash away .\\nRadcliffe's earnings from first five Potter films have been held in trust fund .\",\n          \"Mentally ill inmates in Miami are housed on the \\\"forgotten floor\\\"\\nJudge Steven Leifman says most are there as a result of \\\"avoidable felonies\\\"\\nWhile CNN tours facility, patient shouts: \\\"I am the son of the president\\\"\\nLeifman says the system is unjust and he's fighting for change .\",\n          \"NEW: \\\"I thought I was going to die,\\\" driver says .\\nMan says pickup truck was folded in half; he just has cut on face .\\nDriver: \\\"I probably had a 30-, 35-foot free fall\\\"\\nMinnesota bridge collapsed during rush hour Wednesday .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "summaries"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-d38d638f-231e-4d97-93a7-ef0a01886db2\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pre-trained BART Summaries</th>\n",
       "      <th>Target Summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Harry Potter star Daniel Radcliffe turns 18 on Monday. He gains access to a reported £20 million ($41.1 million) fortune. Radcliffe's earnings from the first five Potter films have been held in a trust fund. Details of how he'll mark his landmark birthday are under wraps.</td>\n",
       "      <td>Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday .\\nYoung actor says he has no plans to fritter his cash away .\\nRadcliffe's earnings from first five Potter films have been held in trust fund .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Judge Steven Leifman is an advocate for justice and the mentally ill. About one-third of all people in Miami-Dade county jails are mentally ill, he says. He says the sheer volume is overwhelming the system. Starting in 2008, many inmates will be sent to a new mental health facility.</td>\n",
       "      <td>Mentally ill inmates in Miami are housed on the \"forgotten floor\"\\nJudge Steven Leifman says most are there as a result of \"avoidable felonies\"\\nWhile CNN tours facility, patient shouts: \"I am the son of the president\"\\nLeifman says the system is unjust and he's fighting for change .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NEW: \"I probably had a 30-, 35-foot free fall,\" survivor Gary Babineau says. NEW: \"My truck was completely face down, pointed toward the ground, and my truck got ripped in half,\" he says. Dr. John Hink says he saw dozens of people lying dazed on an expansive deck. \"It just gave a rumble real quick, and it all just gave way,\" survivor says.</td>\n",
       "      <td>NEW: \"I thought I was going to die,\" driver says .\\nMan says pickup truck was folded in half; he just has cut on face .\\nDriver: \"I probably had a 30-, 35-foot free fall\"\\nMinnesota bridge collapsed during rush hour Wednesday .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d38d638f-231e-4d97-93a7-ef0a01886db2')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-d38d638f-231e-4d97-93a7-ef0a01886db2 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-d38d638f-231e-4d97-93a7-ef0a01886db2');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-b45cdaa7-0a45-4f75-9fa8-c2e8217586f8\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b45cdaa7-0a45-4f75-9fa8-c2e8217586f8')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-b45cdaa7-0a45-4f75-9fa8-c2e8217586f8 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "  <div id=\"id_e66cec94-e22c-4ef5-af58-a1e477013361\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('summaries')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_e66cec94-e22c-4ef5-af58-a1e477013361 button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('summaries');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                              Pre-trained BART Summaries  \\\n",
       "0                                                                       Harry Potter star Daniel Radcliffe turns 18 on Monday. He gains access to a reported £20 million ($41.1 million) fortune. Radcliffe's earnings from the first five Potter films have been held in a trust fund. Details of how he'll mark his landmark birthday are under wraps.   \n",
       "1                                                            Judge Steven Leifman is an advocate for justice and the mentally ill. About one-third of all people in Miami-Dade county jails are mentally ill, he says. He says the sheer volume is overwhelming the system. Starting in 2008, many inmates will be sent to a new mental health facility.   \n",
       "2  NEW: \"I probably had a 30-, 35-foot free fall,\" survivor Gary Babineau says. NEW: \"My truck was completely face down, pointed toward the ground, and my truck got ripped in half,\" he says. Dr. John Hink says he saw dozens of people lying dazed on an expansive deck. \"It just gave a rumble real quick, and it all just gave way,\" survivor says.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                               Target Summaries  \n",
       "0                                                                   Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday .\\nYoung actor says he has no plans to fritter his cash away .\\nRadcliffe's earnings from first five Potter films have been held in trust fund .  \n",
       "1  Mentally ill inmates in Miami are housed on the \"forgotten floor\"\\nJudge Steven Leifman says most are there as a result of \"avoidable felonies\"\\nWhile CNN tours facility, patient shouts: \"I am the son of the president\"\\nLeifman says the system is unjust and he's fighting for change .  \n",
       "2                                                           NEW: \"I thought I was going to die,\" driver says .\\nMan says pickup truck was folded in half; he just has cut on face .\\nDriver: \"I probably had a 30-, 35-foot free fall\"\\nMinnesota bridge collapsed during rush hour Wednesday .  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RGtawibSnfDB"
   },
   "source": [
    "These are good summaries and this is what we will be finetuning bert-base-uncased to do in the next task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13996d20-5eac-4ed8-a2f7-a3cff5e103e9"
   },
   "source": [
    "# Task 2\n",
    "Fine-Tune bert-base-uncased as bert2BERTMK on the cnn_dailymail dataset\n",
    "KM - Kelvin and Manuel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MPbJlcgJnfDB"
   },
   "source": [
    "**Note**: We are using the cnn daily mail dataset because it is tailored for abstractive summarization where as the BBC Xsum dataset is for extreme summarization. We need abstractive because it can help us achieve our goal of multi-sentence summaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jNhxMIFtnfDB"
   },
   "source": [
    "### Loading validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYnsgFvxnfDB"
   },
   "source": [
    "First, we load 10% of the validation dataset for faster validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-02T14:44:24.035767Z",
     "iopub.status.busy": "2024-08-02T14:44:24.035444Z",
     "iopub.status.idle": "2024-08-02T14:44:27.780671Z",
     "shell.execute_reply": "2024-08-02T14:44:27.779815Z",
     "shell.execute_reply.started": "2024-08-02T14:44:24.035740Z"
    },
    "id": "CyqkmZMWnfDC"
   },
   "outputs": [],
   "source": [
    "val_data = datasets.load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"validation[:10%]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3FO5ESocXvlK"
   },
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-02T14:44:27.782371Z",
     "iopub.status.busy": "2024-08-02T14:44:27.781991Z",
     "iopub.status.idle": "2024-08-02T14:44:27.788941Z",
     "shell.execute_reply": "2024-08-02T14:44:27.788010Z",
     "shell.execute_reply.started": "2024-08-02T14:44:27.782335Z"
    },
    "id": "sgTiC0rhMb7C"
   },
   "outputs": [],
   "source": [
    "tokenizer.bos_token = tokenizer.cls_token\n",
    "tokenizer.eos_token = tokenizer.sep_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": false,
    "execution": {
     "iopub.execute_input": "2024-08-02T14:44:27.790677Z",
     "iopub.status.busy": "2024-08-02T14:44:27.790282Z",
     "iopub.status.idle": "2024-08-02T14:44:27.801170Z",
     "shell.execute_reply": "2024-08-02T14:44:27.800275Z",
     "shell.execute_reply.started": "2024-08-02T14:44:27.790642Z"
    },
    "id": "yoN2q0hZUbXN",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size=4\n",
    "encoder_max_length=512\n",
    "decoder_max_length=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": false,
    "execution": {
     "iopub.execute_input": "2024-08-02T14:44:27.802862Z",
     "iopub.status.busy": "2024-08-02T14:44:27.802484Z",
     "iopub.status.idle": "2024-08-02T14:44:27.812908Z",
     "shell.execute_reply": "2024-08-02T14:44:27.812063Z",
     "shell.execute_reply.started": "2024-08-02T14:44:27.802834Z"
    },
    "id": "aj9rL2vnnfDC",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def process_data_to_model_inputs(batch):\n",
    "  # tokenize the inputs and labels\n",
    "    inputs = tokenizer(batch[\"article\"], padding=\"max_length\", truncation=True, max_length=encoder_max_length)\n",
    "    outputs = tokenizer(batch[\"highlights\"], padding=\"max_length\", truncation=True, max_length=decoder_max_length)\n",
    "\n",
    "    batch[\"input_ids\"] = inputs.input_ids\n",
    "    batch[\"attention_mask\"] = inputs.attention_mask\n",
    "    batch[\"labels\"] = outputs.input_ids.copy()\n",
    "\n",
    "    # because BERT automatically shifts the labels, the labels correspond exactly to `decoder_input_ids`.\n",
    "    # We have to make sure that the PAD token is ignored\n",
    "    batch[\"labels\"] = [[-100 if token == tokenizer.pad_token_id else token for token in labels] for labels in\n",
    "                       batch[\"labels\"]]\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "f31406179240401faea5d9cbfcf7c5dd",
      "181d7157a633486ca15ea4d931ff0d40",
      "3a7e8fa3e4444df3af2e6799409b6fb2",
      "2c408a3c748543808b50df8b16f1a218",
      "9b6c6cc270184665b85ea41f2e41a614",
      "25a0342c0aff4b179b9afda4f7fcba30",
      "5ed760e73dae4720b9324557ad0cc886",
      "ad8d1e0eaab14d98aa66ce6e8a55ed09",
      "0edd95ce259647bba92dbffffd605ac6",
      "17842cbd0e0d4d64bde36070ac79a4d3",
      "1494e4712c614b0dbfd3587db2a1eead"
     ]
    },
    "execution": {
     "iopub.execute_input": "2024-08-02T14:44:27.820793Z",
     "iopub.status.busy": "2024-08-02T14:44:27.820516Z",
     "iopub.status.idle": "2024-08-02T14:53:54.765900Z",
     "shell.execute_reply": "2024-08-02T14:53:54.764900Z",
     "shell.execute_reply.started": "2024-08-02T14:44:27.820768Z"
    },
    "id": "oD7RedDEnfDD",
    "outputId": "c8c0daac-055c-48f2-d299-917c90531a09",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f31406179240401faea5d9cbfcf7c5dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/140000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# only use 32 training examples for notebook - DELETE LINE FOR FULL TRAINING\n",
    "train_data = train_data.select(range(140000)) #70000\n",
    "\n",
    "train_data = train_data.map(\n",
    "    process_data_to_model_inputs,\n",
    "    batched=True,\n",
    "    batch_size=batch_size,\n",
    "    remove_columns=[\"article\", \"highlights\", \"id\"]\n",
    ")\n",
    "train_data.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "6ef6ebbdf6ff4c6e9a79c86bfbf3c1fb",
      "86b7ed78fbe44e26ab5d08b546fdbe75",
      "780b0c7b6cd74d2c9f9c416bdd0227cf",
      "54e849d86b2b4bcca72143e9f4bc6412",
      "67c2dfadb4dc4d57939511d3105755f1",
      "d17bdbdff5f44675bc6ce6179fc77486",
      "b4cfcbb631e94dcd84eeb8e6bbb4635e",
      "be97665bd8064569aae841e7f949bf47",
      "9b0d1a21aca342968681f9184802d945",
      "963b1a582b1a4e45837ede3ac76b58b6",
      "cbf19b177a764ae397561e8242e6d390"
     ]
    },
    "execution": {
     "iopub.execute_input": "2024-08-02T14:53:54.768009Z",
     "iopub.status.busy": "2024-08-02T14:53:54.767592Z",
     "iopub.status.idle": "2024-08-02T14:54:00.681583Z",
     "shell.execute_reply": "2024-08-02T14:54:00.680609Z",
     "shell.execute_reply.started": "2024-08-02T14:53:54.767971Z"
    },
    "id": "-szFh5y4nfDD",
    "outputId": "99f8751f-5acf-4983-85e1-5a65b60442a3",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ef6ebbdf6ff4c6e9a79c86bfbf3c1fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1337 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# only use 16 training examples for notebook - DELETE LINE FOR FULL TRAINING\n",
    "#val_data = val_data.select(range(1200))\n",
    "\n",
    "val_data = val_data.map(\n",
    "    process_data_to_model_inputs,\n",
    "    batched=True,\n",
    "    batch_size=batch_size,\n",
    "    remove_columns=[\"article\", \"highlights\", \"id\"]\n",
    ")\n",
    "val_data.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aEjb026cNC38"
   },
   "source": [
    "### Warm-starting the Encoder-Decoder Model\n",
    "The error messages are normal. they show that some weights are being randomly initialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-02T14:54:00.683223Z",
     "iopub.status.busy": "2024-08-02T14:54:00.682866Z",
     "iopub.status.idle": "2024-08-02T14:54:12.169143Z",
     "shell.execute_reply": "2024-08-02T14:54:12.168232Z",
     "shell.execute_reply.started": "2024-08-02T14:54:00.683193Z"
    },
    "id": "tS0UndNoQh8t"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "bert2bert = EncoderDecoderModel.from_encoder_decoder_pretrained(\"bert-base-cased\", \"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-02T14:54:12.170726Z",
     "iopub.status.busy": "2024-08-02T14:54:12.170387Z",
     "iopub.status.idle": "2024-08-02T14:54:12.177409Z",
     "shell.execute_reply": "2024-08-02T14:54:12.176411Z",
     "shell.execute_reply.started": "2024-08-02T14:54:12.170697Z"
    },
    "id": "JD2jv3GkyjR-"
   },
   "outputs": [],
   "source": [
    "# set special tokens\n",
    "bert2bert.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "bert2bert.config.eos_token_id = tokenizer.eos_token_id\n",
    "bert2bert.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# sensible parameters for beam search\n",
    "bert2bert.config.vocab_size = bert2bert.config.decoder.vocab_size\n",
    "bert2bert.config.max_length = 142\n",
    "bert2bert.config.min_length = 56\n",
    "bert2bert.config.no_repeat_ngram_size = 3\n",
    "bert2bert.config.early_stopping = True\n",
    "bert2bert.config.length_penalty = 2.0\n",
    "bert2bert.config.num_beams = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u98CLZiTkgzv"
   },
   "source": [
    "### **Fine-Tuning Warm-Started Encoder-Decoder Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZK_gnIzZgTO"
   },
   "source": [
    "For the `EncoderDecoderModel` framework, we will use the `Seq2SeqTrainingArguments` and the `Seq2SeqTrainer` which have been imported already."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68IHmFYLx09W"
   },
   "source": [
    "### Load rouge for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104,
     "referenced_widgets": [
      "70866980abf0441d8549067b31833aa8",
      "e8364160e7884741a24b5278a678ce5f",
      "3001dc3304be4b4b8901516de4421014",
      "cea12169372e428e9add64e1c7d43bd3",
      "036cdfb75da74ac1a24686756ce4ae29",
      "ddf1df6b775844abba38ceb83fc7596a",
      "d3aaa3ba72e0460ea2793f2b7a1f53cc",
      "c290542b424243f3abeb60cc259b67c2",
      "d47ed14b43a341f5a764bc683071f579",
      "d7d111f804384066a615ecf922ad232a",
      "5015189dabef456ab4de3d1790af8c52"
     ]
    },
    "execution": {
     "iopub.execute_input": "2024-08-02T14:54:12.179366Z",
     "iopub.status.busy": "2024-08-02T14:54:12.178975Z",
     "iopub.status.idle": "2024-08-02T14:54:12.992124Z",
     "shell.execute_reply": "2024-08-02T14:54:12.990962Z",
     "shell.execute_reply.started": "2024-08-02T14:54:12.179332Z"
    },
    "id": "znKQTSkynfDF",
    "outputId": "a09b7fe7-7ab0-4750-c919-61e7e66e7238"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-32-c4923d78972b>:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge = datasets.load_metric(\"rouge\", trust_remote_code=True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70866980abf0441d8549067b31833aa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rouge = datasets.load_metric(\"rouge\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-02T14:54:12.993827Z",
     "iopub.status.busy": "2024-08-02T14:54:12.993494Z",
     "iopub.status.idle": "2024-08-02T14:54:13.001940Z",
     "shell.execute_reply": "2024-08-02T14:54:13.000678Z",
     "shell.execute_reply.started": "2024-08-02T14:54:12.993799Z"
    },
    "id": "69GWFtDjnfDF"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    # all unnecessary tokens are removed\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
    "\n",
    "    return {\n",
    "        \"rouge2_precision\": round(rouge_output.precision, 4),\n",
    "        \"rouge2_recall\": round(rouge_output.recall, 4),\n",
    "        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LAaTxUpdzshF",
    "outputId": "3605173b-5561-4ca9-b20f-2fc64709ea81"
   },
   "source": [
    "**Setting training arguments**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our model training, we referred to Patrick von Platen's demo of the BERT2BERT model to guide our parameter choices. The parameters initially suggested were as follows:\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(  \n",
    "    output_dir=\"./\",  \n",
    "    evaluation_strategy=\"steps\",  \n",
    "    per_device_train_batch_size=batch_size,  \n",
    "    per_device_eval_batch_size=batch_size,  \n",
    "    predict_with_generate=True,  \n",
    "    logging_steps=2,  # set to 1000 for full training  \n",
    "    save_steps=16,  # set to 500 for full training  \n",
    "    eval_steps=4,  # set to 8000 for full training  \n",
    "    warmup_steps=1,  # set to 2000 for full training  \n",
    "    max_steps=16, # delete for full training  \n",
    "    overwrite_output_dir=True,  \n",
    "    save_total_limit=3,  \n",
    "    fp16=True,  \n",
    ")  \n",
    "\n",
    "Due to GPU space limitations, we were unable to perform a grid search for hyperparameter tuning. Therefore, we had to adjust some of these parameters to fit our computational resources and training constraints. We consulted with ChatGPT and Faculty Intern Kweku Yamoah to fine-tune these settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-08-02T14:54:13.004518Z",
     "iopub.status.busy": "2024-08-02T14:54:13.003555Z",
     "iopub.status.idle": "2024-08-02T14:54:13.045596Z",
     "shell.execute_reply": "2024-08-02T14:54:13.044481Z",
     "shell.execute_reply.started": "2024-08-02T14:54:13.004475Z"
    },
    "id": "VEsJl00YnfDF",
    "outputId": "8709775f-fca4-4a92-d78a-d42f80207567"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy = 'steps',\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    logging_steps=1000,  # set to 1000 for full training\n",
    "    save_steps=1000,  # set to 500 for full training\n",
    "    eval_steps=8000,  # set to 8000 for full training\n",
    "    warmup_steps=2000,  # set to 2000 for full training\n",
    "    overwrite_output_dir=True,\n",
    "    save_total_limit=3,\n",
    "    fp16=True,\n",
    "    report_to=\"wandb\"  # Enable W&B logging\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5gD5WainfDG",
    "outputId": "3605173b-5561-4ca9-b20f-2fc64709ea81"
   },
   "source": [
    "**Instantiate trainer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_BY7cTQ_xWHJ",
    "outputId": "8a00ec36-b0f6-4d76-d7ab-2aaa0b15db32"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderDecoderModel(\n",
       "  (encoder): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): BertLMHeadModel(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSdpaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BertAttention(\n",
       "              (self): BertSdpaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (cls): BertOnlyMLMHead(\n",
       "      (predictions): BertLMPredictionHead(\n",
       "        (transform): BertPredictionHeadTransform(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (transform_act_fn): GELUActivation()\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (decoder): Linear(in_features=768, out_features=28996, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "bert2bert.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-02T14:54:13.047449Z",
     "iopub.status.busy": "2024-08-02T14:54:13.047079Z",
     "iopub.status.idle": "2024-08-02T14:54:13.335602Z",
     "shell.execute_reply": "2024-08-02T14:54:13.334722Z",
     "shell.execute_reply.started": "2024-08-02T14:54:13.047419Z"
    },
    "id": "rcMaZZ-unfDG"
   },
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=bert2bert,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ik4hZb2yV-b"
   },
   "source": [
    "Cool! Finally, we start training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZnwRtu_dnfDH"
   },
   "source": [
    "**Train!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "execution": {
     "iopub.execute_input": "2024-08-02T14:54:13.337656Z",
     "iopub.status.busy": "2024-08-02T14:54:13.337248Z"
    },
    "id": "wOO790CrnfDH",
    "outputId": "5733ebfa-0c42-4588-c556-4673e57837b2",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='56001' max='105000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 56001/105000 3:23:26 < 2:58:00, 4.59 it/s, Epoch 1.60/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge2 Precision</th>\n",
       "      <th>Rouge2 Recall</th>\n",
       "      <th>Rouge2 Fmeasure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>2.863700</td>\n",
       "      <td>3.408133</td>\n",
       "      <td>0.062200</td>\n",
       "      <td>0.086100</td>\n",
       "      <td>0.070100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>3.102100</td>\n",
       "      <td>2.937377</td>\n",
       "      <td>0.086400</td>\n",
       "      <td>0.122800</td>\n",
       "      <td>0.098400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>2.905900</td>\n",
       "      <td>2.806555</td>\n",
       "      <td>0.092400</td>\n",
       "      <td>0.138800</td>\n",
       "      <td>0.107700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>2.795200</td>\n",
       "      <td>2.713385</td>\n",
       "      <td>0.096600</td>\n",
       "      <td>0.142300</td>\n",
       "      <td>0.111400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>2.471600</td>\n",
       "      <td>2.675352</td>\n",
       "      <td>0.096500</td>\n",
       "      <td>0.143800</td>\n",
       "      <td>0.111500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>2.471800</td>\n",
       "      <td>2.629470</td>\n",
       "      <td>0.099100</td>\n",
       "      <td>0.150300</td>\n",
       "      <td>0.116200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='256' max='335' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [256/335 09:24 < 02:54, 0.45 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='105000' max='105000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [105000/105000 6:47:46, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge2 Precision</th>\n",
       "      <th>Rouge2 Recall</th>\n",
       "      <th>Rouge2 Fmeasure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>2.863700</td>\n",
       "      <td>3.408133</td>\n",
       "      <td>0.062200</td>\n",
       "      <td>0.086100</td>\n",
       "      <td>0.070100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>3.102100</td>\n",
       "      <td>2.937377</td>\n",
       "      <td>0.086400</td>\n",
       "      <td>0.122800</td>\n",
       "      <td>0.098400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>2.905900</td>\n",
       "      <td>2.806555</td>\n",
       "      <td>0.092400</td>\n",
       "      <td>0.138800</td>\n",
       "      <td>0.107700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>2.795200</td>\n",
       "      <td>2.713385</td>\n",
       "      <td>0.096600</td>\n",
       "      <td>0.142300</td>\n",
       "      <td>0.111400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>2.471600</td>\n",
       "      <td>2.675352</td>\n",
       "      <td>0.096500</td>\n",
       "      <td>0.143800</td>\n",
       "      <td>0.111500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>2.471800</td>\n",
       "      <td>2.629470</td>\n",
       "      <td>0.099100</td>\n",
       "      <td>0.150300</td>\n",
       "      <td>0.116200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>2.404900</td>\n",
       "      <td>2.573213</td>\n",
       "      <td>0.105600</td>\n",
       "      <td>0.157800</td>\n",
       "      <td>0.122700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>2.368000</td>\n",
       "      <td>2.520559</td>\n",
       "      <td>0.104800</td>\n",
       "      <td>0.154000</td>\n",
       "      <td>0.120400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72000</td>\n",
       "      <td>2.075200</td>\n",
       "      <td>2.530947</td>\n",
       "      <td>0.105900</td>\n",
       "      <td>0.156400</td>\n",
       "      <td>0.122400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>2.081700</td>\n",
       "      <td>2.503957</td>\n",
       "      <td>0.106400</td>\n",
       "      <td>0.157700</td>\n",
       "      <td>0.123200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88000</td>\n",
       "      <td>2.056500</td>\n",
       "      <td>2.485168</td>\n",
       "      <td>0.107000</td>\n",
       "      <td>0.157100</td>\n",
       "      <td>0.123100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96000</td>\n",
       "      <td>2.044000</td>\n",
       "      <td>2.466050</td>\n",
       "      <td>0.106700</td>\n",
       "      <td>0.157600</td>\n",
       "      <td>0.123300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104000</td>\n",
       "      <td>2.004500</td>\n",
       "      <td>2.448593</td>\n",
       "      <td>0.105500</td>\n",
       "      <td>0.155700</td>\n",
       "      <td>0.121600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:622: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:642: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=105000, training_loss=2.4967903680710566, metrics={'train_runtime': 24469.1844, 'train_samples_per_second': 17.164, 'train_steps_per_second': 4.291, 'total_flos': 2.5764875329536e+17, 'train_loss': 2.4967903680710566, 'epoch': 3.0})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7zdm50ZotZqb"
   },
   "source": [
    "The model achieves a ROUGE-2 score of **18.22**, which is even a little better than reported in the paper arvixx.  \n",
    "**Be sure to edit this part**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KI7Y5JnjnfDH"
   },
   "source": [
    "# Task 3\n",
    "Evaluate the fine-tuned model with the ROUGE-2 Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZwQIEhKOrJpl"
   },
   "source": [
    "### Evaluation\n",
    "**Rouge?** Rouge is an N-gram Co-Occurrence Statistic  \n",
    "**Brief on Rouge**:\n",
    "ROUGE-2 specifically measures the overlap of bigrams (two-word sequences) between the candidate summary and the reference summary.\n",
    "Key Points of ROUGE-2\n",
    "Bigram Overlap: ROUGE-2 evaluates the similarity based on two-word sequences present in both the candidate and reference summaries.\n",
    "\n",
    "**Interpretation of Scores**\n",
    "Higher ROUGE-2 Score:\n",
    "Indicates better quality of the generated summary.\n",
    "Suggests a higher degree of similarity between the bigrams in the generated summary and the reference summary.\n",
    "Implies that the model is capturing more of the important bi-word sequences from the reference summary [4].  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vGaDltzCnfDH"
   },
   "source": [
    "We finished training our model. Let's now evaluate the model on the test data. We make use of the dataset's handy `.map()` function to generate a summary of each sample of the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_zObLQyonfDH"
   },
   "source": [
    "### Loading test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0HYtRspQnfDI"
   },
   "outputs": [],
   "source": [
    "test_data = datasets.load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LJaIWvJtnfDI"
   },
   "outputs": [],
   "source": [
    "# only use 16 training examples for notebook - DELETE LINE FOR FULL TRAINING\n",
    "test_data = test_data.select(range(5700))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ys8Et9etnfDI"
   },
   "source": [
    "### Load the fine tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oOoSrwWarJAC",
    "outputId": "9324e567-fa21-47fa-a0b4-9b7758fa1463"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderDecoderModel(\n",
       "  (encoder): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): BertLMHeadModel(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (cls): BertOnlyMLMHead(\n",
       "      (predictions): BertLMPredictionHead(\n",
       "        (transform): BertPredictionHeadTransform(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (transform_act_fn): GELUActivation()\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (decoder): Linear(in_features=768, out_features=28996, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-cased\")\n",
    "bert2bertMK = EncoderDecoderModel.from_pretrained(\"/content/checkpoint-105000\")\n",
    "bert2bertMK.gradient_checkpointing_enable()\n",
    "bert2bertMK.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86,
     "referenced_widgets": [
      "6c5c728e769046aa98f7ddef504c76cb",
      "20ed8a8e92584b5596fc5310b953bc9a",
      "2e472c3045a64b8d9602983e70cdd5ff",
      "1d57d4c3532a4957a4dbc6d51c2daaa3",
      "8f5b1de516634d159456cbe645c3671e",
      "4aa4b0d3c02d4a739f77a311439651d9",
      "eb5ed06eab904394813f3ecf8979ead4",
      "d58c7d3c4d4746e4894e6e43ae30e15a",
      "9cd3f9b1a22941f890220f6ac921ad27",
      "a3ffbe1116f247f3a96c7f2fe38ea4a0",
      "a5b60712b92444d5b9cfc23bc743a40c"
     ]
    },
    "id": "HXA6S0DvnfDI",
    "outputId": "f53d9437-0eb2-48bc-c485-8af32782edae"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c5c728e769046aa98f7ddef504c76cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score(precision=0.15225139568423074, recall=0.1674391018626793, fmeasure=0.15436972362742502)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16 #originally 16\n",
    "\n",
    "# map data correctly\n",
    "def generate_summary(batch):\n",
    "    # Tokenizer will automatically set [BOS] <text> [EOS]\n",
    "    # cut off at BERT max length 512\n",
    "    inputs = tokenizer(batch[\"article\"], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    input_ids = inputs.input_ids.to('cuda')\n",
    "    attention_mask = inputs.attention_mask.to('cuda')\n",
    "\n",
    "    with autocast():\n",
    "        outputs = bert2bertMK.generate(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    # all special tokens including will be removed\n",
    "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    batch[\"pred\"] = output_str\n",
    "\n",
    "    return batch\n",
    "\n",
    "results = test_data.map(generate_summary, batched=True, batch_size=batch_size, remove_columns=[\"article\"])\n",
    "\n",
    "pred_str = results[\"pred\"]\n",
    "label_str = results[\"highlights\"]\n",
    "\n",
    "rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
    "\n",
    "print(rouge_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rrGIEVVRnfDI"
   },
   "source": [
    "### Show some sample summaries with the fine-tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0uGc5FhnfDI"
   },
   "source": [
    "**Function to decode tensors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hfn2zTwWnfDJ"
   },
   "outputs": [],
   "source": [
    "def tensor_to_text(tensor_ids):\n",
    "    # Convert tensor IDs to text\n",
    "    text = tokenizer.decode(tensor_ids, skip_special_tokens=True)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nmsWW7_5nfDJ"
   },
   "outputs": [],
   "source": [
    "def summarize_article(article, tokenizer, summarizer):\n",
    "    inputs = tokenizer(article, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    inputs = {key: val.to(\"cuda\") for key, val in inputs.items()}  # Ensure inputs are on GPU\n",
    "    summary_ids = summarizer.generate(\n",
    "        inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        max_length=150,  # Adjust based on your summarization length\n",
    "        num_beams=4,\n",
    "        length_penalty=2.0,\n",
    "        early_stopping=True,\n",
    "        do_sample=True  # Set to True for more diverse summaries\n",
    "    )\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgtGkso2nfDJ"
   },
   "source": [
    "**Generating a few sample summaries with it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jr-ila-UnfDJ"
   },
   "outputs": [],
   "source": [
    "news_articles = [tensor_to_text(example[\"input_ids\"]) for example in train_data.select(range(3))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZKwJ8Y8nfDJ"
   },
   "source": [
    "Summarize the articles and store summaries in summaries list. We set `do_sample` to `True` to avoid greedy decoding and provide a more natural, human-like summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4zgItsMynfDJ"
   },
   "outputs": [],
   "source": [
    "fine_tuned_summaries = []\n",
    "for article in news_articles:\n",
    "    try:\n",
    "        summary = summarize_article(article, tokenizer, bert2bertMK)\n",
    "        fine_tuned_summaries.append(summary)\n",
    "    except Exception as e:\n",
    "        fine_tuned_summaries.append(\"Error, article was too long\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kvCLucannfDK"
   },
   "source": [
    "Create a dataframe containing the fine tuned BERT summaries and the target summaries from our fine-tuning dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9HWoiDkwnfDK"
   },
   "outputs": [],
   "source": [
    "summaries = pd.DataFrame({\n",
    "    'Fine-tuned bert2BERTMK  Summaries': fine_tuned_summaries,\n",
    "    'Target Summaries': target_summaries\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kl4TKy8tnfDK",
    "outputId": "880b29f9-2995-4e27-9b22-c617e8d803a1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Harry Potter star Daniel Radcliffe earns $ 41. 1 million fortune. The 18 - year - old will be able to gamble in a casino or see the horror film \" Hostel : Part II \" Radcliffe\\'s earnings from the first five Potter films have been held in a trust fund. He\\'ll also appear in \" December Boys \" and \" December Boy \"',\n",
       " 'Miami - Dade pretrial detention facility dubbed \" the forgotten floor \" Judge Steven Leifman says inmates with the most severe mental illnesses are incarcerated. Inmate : \" I am the son of the president of the presidency \" Prosecutors say one - third of inmates are mentally ill.',\n",
       " 'NEW : \" I probably had a 30 -, 35 - foot free fall, \" survivor says. \" I could see the whole bridge as it was going down, \" a passenger says. Rescue efforts were controlled and organized, emergency room physician says. The Minnesota bridge fell all the way down, hitting a broken - off section.']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_tuned_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xcptY2RRnfDK",
    "outputId": "551f7ee1-b91f-4837-ce0a-a05233b2000c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday .\\nYoung actor says he has no plans to fritter his cash away .\\nRadcliffe's earnings from first five Potter films have been held in trust fund .\",\n",
       " 'Mentally ill inmates in Miami are housed on the \"forgotten floor\"\\nJudge Steven Leifman says most are there as a result of \"avoidable felonies\"\\nWhile CNN tours facility, patient shouts: \"I am the son of the president\"\\nLeifman says the system is unjust and he\\'s fighting for change .',\n",
       " 'NEW: \"I thought I was going to die,\" driver says .\\nMan says pickup truck was folded in half; he just has cut on face .\\nDriver: \"I probably had a 30-, 35-foot free fall\"\\nMinnesota bridge collapsed during rush hour Wednesday .']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 438
    },
    "id": "UOOSTIrOnfDK",
    "outputId": "91eebedf-9cf3-45a5-fc42-81be66d0b6f6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"summaries\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"Fine-tuned bert2BERTMK  Summaries\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Harry Potter star Daniel Radcliffe earns $ 41. 1 million fortune. The 18 - year - old will be able to gamble in a casino or see the horror film \\\" Hostel : Part II \\\" Radcliffe's earnings from the first five Potter films have been held in a trust fund. He'll also appear in \\\" December Boys \\\" and \\\" December Boy \\\"\",\n          \"Miami - Dade pretrial detention facility dubbed \\\" the forgotten floor \\\" Judge Steven Leifman says inmates with the most severe mental illnesses are incarcerated. Inmate : \\\" I am the son of the president of the presidency \\\" Prosecutors say one - third of inmates are mentally ill.\",\n          \"NEW : \\\" I probably had a 30 -, 35 - foot free fall, \\\" survivor says. \\\" I could see the whole bridge as it was going down, \\\" a passenger says. Rescue efforts were controlled and organized, emergency room physician says. The Minnesota bridge fell all the way down, hitting a broken - off section.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Target Summaries\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Harry Potter star Daniel Radcliffe gets \\u00a320M fortune as he turns 18 Monday .\\nYoung actor says he has no plans to fritter his cash away .\\nRadcliffe's earnings from first five Potter films have been held in trust fund .\",\n          \"Mentally ill inmates in Miami are housed on the \\\"forgotten floor\\\"\\nJudge Steven Leifman says most are there as a result of \\\"avoidable felonies\\\"\\nWhile CNN tours facility, patient shouts: \\\"I am the son of the president\\\"\\nLeifman says the system is unjust and he's fighting for change .\",\n          \"NEW: \\\"I thought I was going to die,\\\" driver says .\\nMan says pickup truck was folded in half; he just has cut on face .\\nDriver: \\\"I probably had a 30-, 35-foot free fall\\\"\\nMinnesota bridge collapsed during rush hour Wednesday .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "summaries"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-09fe5356-d223-4f8e-b1a2-86a41ca40ef5\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fine-tuned bert2BERTMK  Summaries</th>\n",
       "      <th>Target Summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Harry Potter star Daniel Radcliffe earns $ 41. 1 million fortune. The 18 - year - old will be able to gamble in a casino or see the horror film \" Hostel : Part II \" Radcliffe's earnings from the first five Potter films have been held in a trust fund. He'll also appear in \" December Boys \" and \" December Boy \"</td>\n",
       "      <td>Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday .\\nYoung actor says he has no plans to fritter his cash away .\\nRadcliffe's earnings from first five Potter films have been held in trust fund .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Miami - Dade pretrial detention facility dubbed \" the forgotten floor \" Judge Steven Leifman says inmates with the most severe mental illnesses are incarcerated. Inmate : \" I am the son of the president of the presidency \" Prosecutors say one - third of inmates are mentally ill.</td>\n",
       "      <td>Mentally ill inmates in Miami are housed on the \"forgotten floor\"\\nJudge Steven Leifman says most are there as a result of \"avoidable felonies\"\\nWhile CNN tours facility, patient shouts: \"I am the son of the president\"\\nLeifman says the system is unjust and he's fighting for change .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NEW : \" I probably had a 30 -, 35 - foot free fall, \" survivor says. \" I could see the whole bridge as it was going down, \" a passenger says. Rescue efforts were controlled and organized, emergency room physician says. The Minnesota bridge fell all the way down, hitting a broken - off section.</td>\n",
       "      <td>NEW: \"I thought I was going to die,\" driver says .\\nMan says pickup truck was folded in half; he just has cut on face .\\nDriver: \"I probably had a 30-, 35-foot free fall\"\\nMinnesota bridge collapsed during rush hour Wednesday .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-09fe5356-d223-4f8e-b1a2-86a41ca40ef5')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-09fe5356-d223-4f8e-b1a2-86a41ca40ef5 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-09fe5356-d223-4f8e-b1a2-86a41ca40ef5');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-55c951b0-d4d7-4cfa-ba07-3706ecd70e04\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-55c951b0-d4d7-4cfa-ba07-3706ecd70e04')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-55c951b0-d4d7-4cfa-ba07-3706ecd70e04 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "  <div id=\"id_a200fbfb-3799-4245-8260-a2c1a29ac350\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('summaries')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_a200fbfb-3799-4245-8260-a2c1a29ac350 button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('summaries');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                        Fine-tuned bert2BERTMK  Summaries  \\\n",
       "0  Harry Potter star Daniel Radcliffe earns $ 41. 1 million fortune. The 18 - year - old will be able to gamble in a casino or see the horror film \" Hostel : Part II \" Radcliffe's earnings from the first five Potter films have been held in a trust fund. He'll also appear in \" December Boys \" and \" December Boy \"   \n",
       "1                                 Miami - Dade pretrial detention facility dubbed \" the forgotten floor \" Judge Steven Leifman says inmates with the most severe mental illnesses are incarcerated. Inmate : \" I am the son of the president of the presidency \" Prosecutors say one - third of inmates are mentally ill.   \n",
       "2                  NEW : \" I probably had a 30 -, 35 - foot free fall, \" survivor says. \" I could see the whole bridge as it was going down, \" a passenger says. Rescue efforts were controlled and organized, emergency room physician says. The Minnesota bridge fell all the way down, hitting a broken - off section.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                               Target Summaries  \n",
       "0                                                                   Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday .\\nYoung actor says he has no plans to fritter his cash away .\\nRadcliffe's earnings from first five Potter films have been held in trust fund .  \n",
       "1  Mentally ill inmates in Miami are housed on the \"forgotten floor\"\\nJudge Steven Leifman says most are there as a result of \"avoidable felonies\"\\nWhile CNN tours facility, patient shouts: \"I am the son of the president\"\\nLeifman says the system is unjust and he's fighting for change .  \n",
       "2                                                           NEW: \"I thought I was going to die,\" driver says .\\nMan says pickup truck was folded in half; he just has cut on face .\\nDriver: \"I probably had a 30-, 35-foot free fall\"\\nMinnesota bridge collapsed during rush hour Wednesday .  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8a5pO5amnfDK"
   },
   "source": [
    "We see that our model performs great on summaries with a Rouge-2 score of **15.5.** which is somewhat close to what Patrick Von Platen reports on hugging face **18.22**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0e3004b6-ab40-4d0d-8f08-ff64374c2248"
   },
   "source": [
    "# Task 4\n",
    "Save the fine-tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jCd0UDcXnfDL"
   },
   "source": [
    "###  Save Fine-Tuned Model\n",
    "Save the model and tokenizer to the 'model' folder in the current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1419d2cb-a9f6-4ad8-a219-0fcbe212f4db"
   },
   "outputs": [],
   "source": [
    "bert2bert.save_pretrained('./model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZQS9FZK2nfDL"
   },
   "source": [
    "### Save Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WlUYY50vKXTY",
    "outputId": "f6d15b9d-408c-41ee-840d-b1a882a43a72"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./model/tokenizer_config.json',\n",
       " './model/special_tokens_map.json',\n",
       " './model/vocab.txt',\n",
       " './model/added_tokens.json',\n",
       " './model/tokenizer.json')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('./model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CNftMjILnfDL"
   },
   "source": [
    "### Split model into parts for GitHub upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4w8lM_e4nfDL"
   },
   "outputs": [],
   "source": [
    "def split_file(file_path, chunk_size_mb=50):\n",
    "    chunk_size = chunk_size_mb * 1024 * 1024  # Convert MB to bytes\n",
    "    with open(file_path, 'rb') as f:\n",
    "        chunk = f.read(chunk_size)\n",
    "        i = 0\n",
    "        while chunk:\n",
    "            with open(f\"{file_path}.part{i}\", 'wb') as chunk_file:\n",
    "                chunk_file.write(chunk)\n",
    "            i += 1\n",
    "            chunk = f.read(chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W_njL76wnfDM"
   },
   "outputs": [],
   "source": [
    "model_dir = './model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VPMGLQMVnfDM"
   },
   "outputs": [],
   "source": [
    "# Split only the .bin file\n",
    "for filename in os.listdir(model_dir):\n",
    "    if filename.endswith('.bin'):  # Only split .bin files\n",
    "        file_path = os.path.join(model_dir, filename)\n",
    "        split_file(file_path, chunk_size_mb=50)  # Split into 50 MB chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oiWiFLH-nfDM",
    "outputId": "c1c3d54d-ddd0-487d-c37c-197cf62aa212"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved and zipped at ./model.zip\n"
     ]
    }
   ],
   "source": [
    "zip_file_name = './model.zip'\n",
    "shutil.make_archive(zip_file_name.replace('.zip', ''), 'zip', model_dir)\n",
    "\n",
    "print(f\"Model saved and zipped at {zip_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RdCxRUSlWThJ",
    "outputId": "9c44d5a4-56ff-4a45-a5a3-31abd99c7964"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our model is trained and saved we can use it in our deployed application to summarize news articles in real-time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6aZZDQ5ynfDM"
   },
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mGM6FlZCnfDM"
   },
   "source": [
    "**Bibliography**  \n",
    "[1]Snowflake Inc. 2024. Connect Streamlit to Google Cloud Storage - Streamlit Docs. docs.streamlit.io. Retrieved July 19, 2024 from https://docs.streamlit.io/develop/tutorials/databases/gcs  \n",
    "[2]Keras Team. 2024. Keras documentation: InceptionV3. keras.io. Retrieved July 19, 2024 from https://keras.io/api/applications/inceptionv3/  \n",
    "[3]TensorFlow. 2024. Load video data | TensorFlow Core. TensorFlow. Retrieved July 19, 2024 from https://www.tensorflow.org/tutorials/load_data/video#create_frames_from_each_video_file  \n",
    "[4]TensorFlow. 2024. tf.keras.applications.inception_v3.decode_predictions | TensorFlow v2.16.1. TensorFlow. Retrieved July 19, 2024 from https://www.tensorflow.org/api_docs/python/tf/keras/applications/inception_v3/decode_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2DeoMQnnfCr"
   },
   "source": [
    "# Phase 5         \n",
    "Deploy Functional App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1a765bda"
   },
   "source": [
    "### Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A9jXrb4BnfCs"
   },
   "source": [
    "This notebook addresses Phase 4 of the What's The Hot Topic in Topic Town? project: **Deploy Functional App**.  \n",
    "The self-created rubric, in our repository, explains the requirement for a proper execution of this phase as seen below.   \n",
    "**Description:** Use Streamlit to build an interactive and responsive web application for project functionality\n",
    "\n",
    "**Note:** This notebook **only** shows the code and instructions for deployment, specifically writing the Streamlit app. As such, you cannot run this notebook. You must have already build your Spider with scrapy before attempting to write a Streamlit app as the Streamlit  integrates all the elements of **Phase 1-5** in one working system. If you have not built your Spider yet, you can learn how to build one using this link: [Scrapy Tutorial](https://docs.scrapy.org/en/latest/intro/tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wA3pjxAnfCt"
   },
   "source": [
    "### Repository Link\n",
    "\n",
    "Here is a link to our repository:\n",
    "\n",
    "[What's The Hot Topic In Town?](https://github.com/kelvin-ahiakpor/Whats.The.Hot.Topic.In.Town)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "de02dd80"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-02T14:41:22.913248Z",
     "iopub.status.busy": "2024-08-02T14:41:22.912735Z",
     "iopub.status.idle": "2024-08-02T14:42:42.089722Z",
     "shell.execute_reply": "2024-08-02T14:42:42.088757Z",
     "shell.execute_reply.started": "2024-08-02T14:41:22.913213Z"
    },
    "id": "w67vkz3KP9eZ"
   },
   "outputs": [],
   "source": [
    "import phase2_phase3 as news\n",
    "import streamlit as st\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import os\n",
    "import sys\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "from transformers import TFAutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "de02dd80"
   },
   "source": [
    "### Function to reassmble split model files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reassemble_file(chunk_prefix, output_file_path, input_dir='model'):\n",
    "    chunk_files = sorted([f for f in os.listdir(input_dir) if f.startswith(chunk_prefix)])\n",
    "    with open(output_file_path, 'wb') as output_file:\n",
    "        for chunk_file_name in chunk_files:\n",
    "            with open(os.path.join(input_dir, chunk_file_name), 'rb') as chunk_file:\n",
    "                output_file.write(chunk_file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "de02dd80"
   },
   "source": [
    "### Function to load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the model from the combined bytes\n",
    "def load_model():\n",
    "    output_file_path = '../bert2bertMK/model.safetensors'  # Path for the reassembled model\n",
    "    chunk_prefix = 'model.safetensors_chunk_'\n",
    "    reassemble_file(chunk_prefix, output_file_path, input_dir='../bert2bertMK/model')\n",
    "\n",
    "    # Load the model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained('../bert2bertMK')\n",
    "    model = TFAutoModelForSeq2SeqLM.from_pretrained('../bert2bertMK')\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "de02dd80"
   },
   "source": [
    "### Function to summarize an article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_article(article_text, model, tokenizer):\n",
    "    if model is None or tokenizer is None:\n",
    "        return \"Error: Model or tokenizer not loaded properly.\"\n",
    "    \n",
    "    try:\n",
    "        inputs = tokenizer.encode(\"summarize: \" + article_text, return_tensors=\"tf\", max_length=512, truncation=True)\n",
    "        outputs = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "        summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return summary\n",
    "    except Exception as e:\n",
    "        return f\"Error during summarization: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streamlit app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "menu = [\"HOME\", \"SEARCH\"]\n",
    "choice = st.sidebar.selectbox(\"Menu\", menu)\n",
    "\n",
    "st.sidebar.title(\"About\")\n",
    "st.sidebar.info(\n",
    "    \"\"\"\n",
    "     This app informs users about the hottest trending topics in their country and provides sentiment analysis and concise summaries. Users can access the original articles as well.\n",
    "     \n",
    "    GitHub: [What'sTheHotTopicInTown?](https://github.com/kelvin-ahiakpor/Whats.The.Hot.Topic.In.Town)\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "if choice ==  \"HOME\":\n",
    "    # Main title\n",
    "    st.title(\"Welcome to TrendWatch!\")\n",
    "\n",
    "    # Catchy writeup\n",
    "    st.markdown(\"\"\"\n",
    "    ### Experience The Future of News! \n",
    "    Your go-to app for discovering the hottest trending topics in your country. Our cutting-edge technology ensures you're always in the loop with what's happening around you.\n",
    "\n",
    "    **Why Choose TrendWatch?**\n",
    "\n",
    "    - **Instant Updates**: Get the latest trending topics as they happen, scraped in real-time using the powerful Scrapy library.\n",
    "    - **Deep Insights**: Understand the public mood with our advanced sentiment analysis powered by the state-of-the-art Distill-BERT model.\n",
    "    - **Quick Reads**: No time to read long articles? Our Bert2BertMK model generates concise summaries, so you get the gist in no time.\n",
    "    - **Stay Informed**: Dive deeper into topics that interest you with direct links to the full articles.\n",
    "\n",
    "    **Stay Ahead of The Curve**\n",
    "\n",
    "    With TrendWatch, you'll never miss out on the trends that matter. Whether it's breaking news, viral topics, or the latest buzz, we've got you covered. Our sleek and user-friendly interface ensures you get all the information you need at a glance.\n",
    "\n",
    "    Stay up to date, more details on our community will be provided soon.\n",
    "    \"\"\")\n",
    "\n",
    "    # Contact information\n",
    "    st.markdown(\"\"\"\n",
    "    **Contact Us:**\n",
    "    - Emmanuel Acquaye: [emmanuel.acquaye@ashesi.edu.gh](mailto:emmanuel.acquaye@ashesi.edu.gh)\n",
    "    - Kelvin Ahiakpor: [kelvin.ahiakpor@ashesi.edu.gh](mailto:kelvin.ahiakpor@ashesi.edu.gh)\n",
    "    - **Student IDs**: 10112026, 47822026\n",
    "    - **GitHub**: [WHAT'S THE HOT TOPIC IN TOWN?](https://github.com/kelvin-ahiakpor/Whats.The.Hot.Topic.In.Town)\n",
    "    \"\"\")\n",
    "\n",
    "    # Start Searching Now button\n",
    "    if st.button('Start Searching Now'):\n",
    "        st.session_state.page = 'search'\n",
    "        \n",
    "elif choice == \"SEARCH\":\n",
    "    # Main title\n",
    "    st.title(\"Search for Trending Topics\")\n",
    "\n",
    "    with st.expander(\"HOW TO USE?\"):\n",
    "        st.markdown(\n",
    "            \"\"\"\n",
    "            **INSTRUCTIONS:**\n",
    "            - Type in the African Country of choice and click the Get Trending News Button.\n",
    "            - The topics are then displayed to you with their summaries and sentiments\n",
    "            \n",
    "            **Contact Information:**\n",
    "            - Email: emmanuel.acquaye@ashesi.edu.gh, kelvin.ahiakpor@ashesi.edu.gh\n",
    "            - Student ID: 10112026, 47822026\n",
    "            \n",
    "            - GitHub: [WHAT'S THE HOT TRENDING TOPIC IN TOWN](https://github.com/kelvin-ahiakpor/Whats.The.Hot.Topic.In.Town)\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "    top_articles = None\n",
    "\n",
    "    def run_scrapy_script(country):\n",
    "        result = subprocess.run([sys.executable, \"RunSpider.py\", country], capture_output=True, text=True)\n",
    "        print(\"scraped\")\n",
    "        return result.stdout\n",
    "\n",
    "    st.subheader(\"FIND THE TOP TRENDS OF AFRICAN COUNTRIES\")\n",
    "    country = st.text_input(\"Enter Country For News\")\n",
    "    scrape_button = st.button(\"Get Trending News\")\n",
    "\n",
    "    if scrape_button and country:\n",
    "        with st.spinner('Scraping data...'):\n",
    "            output = run_scrapy_script(country)\n",
    "        st.success(\"Scraping completed!\")\n",
    "        \n",
    "        conn = sqlite3.connect('newsData.db')\n",
    "        newsdata = news.load_and_clean_data(conn)\n",
    "        top_terms = news.get_top_terms(newsdata)\n",
    "        top_articles = news.calculate_relevance(newsdata, top_terms)\n",
    "        model, tokenizer = load_model()\n",
    "\n",
    "        if model is None or tokenizer is None:\n",
    "            st.error(\"Failed to load the model or tokenizer. Please check the console for more information.\")\n",
    "        elif top_articles is not None and not top_articles.empty:\n",
    "            st.subheader(\"TOP 10 TRENDING TOPICS\")\n",
    "            \n",
    "            article_titles = top_articles['TITLE'].tolist()\n",
    "            \n",
    "            for i in range(len(article_titles)):\n",
    "                st.subheader(str(i+1) + \". \" + article_titles[i])\n",
    "                article_content = top_articles[top_articles['TITLE'] == article_titles[i]]['BODY'].values[0]\n",
    "                summary = summarize_article(article_content, model, tokenizer)\n",
    "                st.write(summary)\n",
    "                st.write()\n",
    "                link = top_articles[top_articles['TITLE'] == article_titles[i]]['URL'].values[0]\n",
    "                sentiment = news.analyze_sentiment(article_content)\n",
    "                if sentiment == \"POSITIVE\":\n",
    "                    st.write(\"Sentiment Analysis:  😊 \", sentiment)\n",
    "                else:\n",
    "                    st.write(\"Sentiment Analysis: 😡 \", sentiment)\n",
    "                    \n",
    "                st.write(\"Link To Article: \", link)\n",
    "        else:\n",
    "            st.write(\"No articles found or error in processing data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thanks for reading**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00a8884ce6454401bb291b39ca8e0640": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0126020315744923ba10f565fd625764": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "02a9c20377b84d739804fb3ec99736ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "058f4eb2dc7641459c026c3ada509254": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8ff4d107dd4b42a0a2f8052cf0ebf575",
       "IPY_MODEL_8e5c264f5c374bacbffea5e87b8e0126",
       "IPY_MODEL_353d35c2f1264e41b1f732d52cf2113f"
      ],
      "layout": "IPY_MODEL_88daaab7bdba4ca587dfba03945bf762"
     }
    },
    "0db04860235b468eaa0d67a4e0b75847": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0e2ad5f651f84f9987ea84a6830a6ed6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7cb633e044c84c43b84ef52d8ed97025",
      "placeholder": "​",
      "style": "IPY_MODEL_13b15a58c86645369040862f18973e14",
      "value": " 629/629 [00:00&lt;00:00, 11.5kB/s]"
     }
    },
    "13a7b31714e541f3bef75da2e1179e33": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c8e158e13059418c8d35b15efe8f6674",
      "placeholder": "​",
      "style": "IPY_MODEL_eb779fa3d1a34ea087d0a92e3dff67eb",
      "value": " 268M/268M [00:03&lt;00:00, 98.9MB/s]"
     }
    },
    "13b15a58c86645369040862f18973e14": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2088fac2762041ff8fffeee2293dd73a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2346d4eb65f6428095056b1bb0afbdad": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "292aa3e3c77f4396a83fcd26152ca2ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3d8e59fd90cf40369de1c7c700a0236e",
      "placeholder": "​",
      "style": "IPY_MODEL_a19fe100eca94cc3aa3a6a8921a2e936",
      "value": "vocab.txt: 100%"
     }
    },
    "343499130b724c7a86412ac994eec587": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "353d35c2f1264e41b1f732d52cf2113f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7e4c8e1550f049068ca5c20263a9e6d8",
      "placeholder": "​",
      "style": "IPY_MODEL_705ffb66e5e44ba6a698569c40fdabc0",
      "value": " 48.0/48.0 [00:00&lt;00:00, 1.27kB/s]"
     }
    },
    "3d8e59fd90cf40369de1c7c700a0236e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "419c37febb194e95ba1ce6a10e02d702": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_292aa3e3c77f4396a83fcd26152ca2ec",
       "IPY_MODEL_f7a05fe737624af88bafc343f8a8d8e9",
       "IPY_MODEL_4ff83a4439bd4420a42f5096fde41e2d"
      ],
      "layout": "IPY_MODEL_fbc93bf0ecd94d339cdf2b80be2c2f7a"
     }
    },
    "4e68225ed1334e99afdc90ae6422a230": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4ff83a4439bd4420a42f5096fde41e2d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5ca9c213a9954b3c843828f9f86c0807",
      "placeholder": "​",
      "style": "IPY_MODEL_d26e3d05d20d48639367f4347d0c6e50",
      "value": " 232k/232k [00:00&lt;00:00, 1.91MB/s]"
     }
    },
    "5ca9c213a9954b3c843828f9f86c0807": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5ff8b153e480435f9277b384e13c2327": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "641f2d82451945708719f1ecab612bc7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "702dd4ee8f7844828db5e9faf4deb5a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "705ffb66e5e44ba6a698569c40fdabc0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "716f02820db448c68b73c5380a20dc0b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7cb633e044c84c43b84ef52d8ed97025": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7e4c8e1550f049068ca5c20263a9e6d8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7f4a1e468221468b99b0b38817cbbc83": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c2c3d760acd749db992e7d9d83d28d20",
      "placeholder": "​",
      "style": "IPY_MODEL_343499130b724c7a86412ac994eec587",
      "value": "model.safetensors: 100%"
     }
    },
    "88daaab7bdba4ca587dfba03945bf762": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8e5c264f5c374bacbffea5e87b8e0126": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5ff8b153e480435f9277b384e13c2327",
      "max": 48,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_02a9c20377b84d739804fb3ec99736ad",
      "value": 48
     }
    },
    "8ff4d107dd4b42a0a2f8052cf0ebf575": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_716f02820db448c68b73c5380a20dc0b",
      "placeholder": "​",
      "style": "IPY_MODEL_a5ab3da857eb4632b16bf4ea370ba662",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "9fd470bbe627496690affb852ab46b07": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a19fe100eca94cc3aa3a6a8921a2e936": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a24bd63ca7534fc39100939b1d09047d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7f4a1e468221468b99b0b38817cbbc83",
       "IPY_MODEL_c481a43995a84b889ff893ca7dfe155b",
       "IPY_MODEL_13a7b31714e541f3bef75da2e1179e33"
      ],
      "layout": "IPY_MODEL_0db04860235b468eaa0d67a4e0b75847"
     }
    },
    "a2d3e73f49174038bc66c5ec76100136": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_db1bee522cb845598c068e0b5213adea",
      "placeholder": "​",
      "style": "IPY_MODEL_9fd470bbe627496690affb852ab46b07",
      "value": "config.json: 100%"
     }
    },
    "a5ab3da857eb4632b16bf4ea370ba662": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ac85b1d108bf49bb90e06ed1152bc3c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a2d3e73f49174038bc66c5ec76100136",
       "IPY_MODEL_e258fea5193b4c47889a84a67b6a4f21",
       "IPY_MODEL_0e2ad5f651f84f9987ea84a6830a6ed6"
      ],
      "layout": "IPY_MODEL_00a8884ce6454401bb291b39ca8e0640"
     }
    },
    "c2c3d760acd749db992e7d9d83d28d20": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c481a43995a84b889ff893ca7dfe155b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2088fac2762041ff8fffeee2293dd73a",
      "max": 267832558,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4e68225ed1334e99afdc90ae6422a230",
      "value": 267832558
     }
    },
    "c8e158e13059418c8d35b15efe8f6674": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d26e3d05d20d48639367f4347d0c6e50": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "db1bee522cb845598c068e0b5213adea": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e258fea5193b4c47889a84a67b6a4f21": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_641f2d82451945708719f1ecab612bc7",
      "max": 629,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0126020315744923ba10f565fd625764",
      "value": 629
     }
    },
    "eb779fa3d1a34ea087d0a92e3dff67eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f7a05fe737624af88bafc343f8a8d8e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2346d4eb65f6428095056b1bb0afbdad",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_702dd4ee8f7844828db5e9faf4deb5a3",
      "value": 231508
     }
    },
    "fbc93bf0ecd94d339cdf2b80be2c2f7a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
